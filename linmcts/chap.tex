\chapter{Recherche arborescente linéaire}
\label{chap:linmcts}                   % étiquette pour renvois (à compléter!)

\section{Estimation du score de tous les résumés}

On a des représentations vectorielles $\psi(s)$ des noeuds
de l'arbre de résumé.
Les représentations sont issues de l'encodeur.
On cherche à trouver le mapping linéaire $\theta$ minimisant

\begin{equation}
    \left( R(s, \hat(s)) - \langle \theta, \psi(s)\rangle \right)^2
\end{equation}


\section{Recherche d'une approximation du score de tous les résumés}

\commentaire{Juste un copier-coller d'un document que j'avais préparé à la fin de
    l'été.}

The experiments were done using 25 000 random training documents from the CNN/DailyMail dataset.
For the pre-training, a fully connected layer took as input 3 sentence embeddings and was tasked with predicting their relative ROUGE-\{1,2,L\} scores.
For each document in the pre-training phase, 250 summaries of 3 sentences were randomly sampled and their ROUGE scores were used as targets.
The pre-training batch size was of 64.

\tikzsetnextfilename{linMcts_pretraining}
\begin{center}
    \begin{tikzpicture}
        \begin{axis}[title={Importance of pre-training, $\alpha = 0.1$}, grid style={dashed,gray!50}, axis y line*=left, axis x line*=bottom, every axis plot/.append style={line width=1.5pt, mark size=0pt, font=\huge}, name=plot0, xshift=-.1\textwidth, y tick label style={/pgf/number format/fixed zerofill}, ylabel={Inferred ROUGE}, xlabel={n samples}, width=\textwidth, legend style={at={(1,0.1)},anchor=south east}, smooth, xmin=0, xmax=500, ymin=0.2, ymax=0.45]
            \addplot[blue, ylabel near ticks, line width=3pt] table[x=x0, y=y0, col sep=comma]{linmcts/linmctsExpResults/pre_training_experiment/alpha_0.1--1000--pretrained.csv};
            \addplot[red, ylabel near ticks, line width=3pt] table[x=x0, y=y0, col sep=comma]{linmcts/linmctsExpResults/pre_training_experiment/alpha_0.1--100--pretrained.csv};
            \addplot[green, ylabel near ticks, line width=3pt] table[x=x0, y=y0, col sep=comma]{linmcts/linmctsExpResults/pre_training_experiment/alpha_0.1--1000--raw.csv};
            \addplot[black, dotted, ylabel near ticks, line width=3pt] table[x=x0, y=y3, col sep=comma]{linmcts/linmctsExpResults/pre_training_experiment/alpha_0.1--1000--raw.csv};
            \legend{1000 warmup steps, 100 warmup steps, random, maximum}
        \end{axis}
    \end{tikzpicture}
\end{center}

\noindent\textbf{Analysis}
The pretraining works as intended.
After 100 warmup steps, the score prediction head has merely learned which range is adequate for ROUGE-\{1,2,L\}
After 1000 warmup steps, it has however begun to better correlate embeddings to their scores, enabling a much improved MCTS exploration.
Less massive batches could be interesting to explore (i.e. instead of sampling 250 summaries per document, move to a more reasonable 16 summaries in a more classical online learning fashion).


\tikzsetnextfilename{linMcts_exploration}
\begin{center}
    \begin{tikzpicture}
        \begin{axis}[title={Exploring the exploration, all 10 000 warmup steps}, grid style={dashed,gray!50}, axis y line*=left, axis x line*=bottom, every axis plot/.append style={line width=1.5pt, mark size=0pt, font=\huge}, name=plot0, xshift=-.1\textwidth, y tick label style={/pgf/number format/fixed zerofill}, ylabel={Inferred ROUGE}, xlabel={n samples}, width=\textwidth, legend style={at={(1,0.1)},anchor=south east}, smooth, xmin=0, xmax=500, ymin=0.2, ymax=0.45]
            \addplot[blue, ylabel near ticks, line width=3pt] table[x=x0, y=y0, col sep=comma]{linmcts/linmctsExpResults/alpha_experiment/alpha_0.1--1000--pretrained.csv};
            \addplot[red, ylabel near ticks, line width=3pt] table[x=x0, y=y0, col sep=comma]{linmcts/linmctsExpResults/alpha_experiment/alpha_1.0--1000--pretrained.csv};
            \addplot[green, ylabel near ticks, line width=3pt] table[x=x0, y=y0, col sep=comma]{linmcts/linmctsExpResults/alpha_experiment/alpha_0.01--1000--pretrained.csv};
            \addplot[cyan, ylabel near ticks, line width=3pt] table[x=x0, y=y0, col sep=comma]{linmcts/linmctsExpResults/alpha_experiment/alpha_10.0--1000--pretrained.csv};
            \addplot[black, dotted, ylabel near ticks, line width=3pt] table[x=x0, y=y3, col sep=comma]{linmcts/linmctsExpResults/pre_training_experiment/alpha_0.1--1000--raw.csv};
            \legend{$\alpha = 0.1$, $\alpha = 1.0$, $\alpha = 0.01$, $\alpha = 10.0$, maximum}
        \end{axis}
    \end{tikzpicture}
\end{center}

\noindent\textbf{Analysis}
There seems to be a sole winner of $\alpha=0.1$ as the best exploration parameter for our experiments.
It seems this parameter is absolutely crucial as it really influences the performance in both the earlier and later stages of the MCTS.


\tikzsetnextfilename{linMcts_less20}
\begin{tikzpicture}[baseline]
    \begin{axis}[grid style={dashed,gray!50}, axis y line*=left, axis x line*=bottom, every axis plot/.append style={line width=1.5pt, mark size=0pt, font=\Large}, width=0.45\textwidth,
            height=0.6\textwidth, name=plot0, xshift=-.1\textwidth, y tick label style={/pgf/number format/fixed zerofill},xmin=0.0, xmax=500.0, ymin=0.20, ymax=0.45, ylabel={Inferred ROUGE}, xlabel={n samples}, legend style={at={(1,0.05), nodes={scale=0.5, transform shape}},anchor=south east}, smooth, title={$|d| \leq 20$}]
        \addplot[yellow, ylabel near ticks, line width=3pt] table[x=x0, y=y0, col sep=comma]{linmcts/linmctsExpResults/document_length_experiment/alpha_0.01--1000--pretrained--c1.csv};
        \addplot[blue, ylabel near ticks, line width=3pt] table[x=x0, y=y0, col sep=comma]{linmcts/linmctsExpResults/document_length_experiment/alpha_0.1--1000--pretrained--c1.csv};
        \addplot[red, ylabel near ticks, line width=3pt] table[x=x0, y=y0, col sep=comma]{linmcts/linmctsExpResults/document_length_experiment/alpha_1.0--1000--pretrained--c1.csv};
        \addplot[black, dotted, ylabel near ticks, line width=3pt] table[x=x0, y=y3, col sep=comma]{linmcts/linmctsExpResults/document_length_experiment/alpha_0.1--1000--pretrained--c1.csv};
        \legend{$\alpha=0.01$, $\alpha=0.1$, $\alpha=1.0$, maximum}

    \end{axis}
\end{tikzpicture}
\tikzsetnextfilename{linMcts_20to35}
\begin{tikzpicture}[baseline]
    \begin{axis}[grid style={dashed,gray!50}, axis y line*=left, axis x line*=bottom, every axis plot/.append style={line width=1.5pt, mark size=0pt, font=\huge}, width=.45\textwidth,
            height=0.6\textwidth, name=plot0, xshift=-.1\textwidth, y tick label style={/pgf/number format/fixed zerofill},xmin=0.0, xmax=500.0, ymin=0.20, ymax=0.45, xlabel={n samples}, legend style={at={(0.9,0.1)},anchor=south east}, smooth, title={$20 < |d| < 35$}, ymajorticks=false]
        \addplot[yellow, ylabel near ticks, line width=3pt] table[x=x0, y=y0, col sep=comma]{linmcts/linmctsExpResults/document_length_experiment/alpha_0.01--1000--pretrained--c2.csv};
        \addplot[blue, ylabel near ticks, line width=3pt] table[x=x0, y=y0, col sep=comma]{linmcts/linmctsExpResults/document_length_experiment/alpha_0.1--1000--pretrained--c2.csv};
        \addplot[red, ylabel near ticks, line width=3pt] table[x=x0, y=y0, col sep=comma]{linmcts/linmctsExpResults/document_length_experiment/alpha_1.0--1000--pretrained--c2.csv};
        \addplot[black, dotted, ylabel near ticks, line width=3pt] table[x=x0, y=y3, col sep=comma]{linmcts/linmctsExpResults/document_length_experiment/alpha_0.1--1000--pretrained--c2.csv};
    \end{axis}
\end{tikzpicture}
\tikzsetnextfilename{linMcts_more35}
\begin{tikzpicture}[baseline]
    \begin{axis}[grid style={dashed,gray!50}, axis y line*=right, axis x line*=bottom, every axis plot/.append style={line width=1.5pt, mark size=0pt, font=\huge}, width=.4\textwidth,
            height=0.6\textwidth, name=plot0, xshift=-.1\textwidth, y tick label style={/pgf/number format/fixed zerofill},xmin=0.0, xmax=500.0, ymin=0.20, ymax=0.45, xlabel={n samples}, legend style={at={(1,0.1)},anchor=south east}, smooth, title={$35 \leq |d|$}]
        \addplot[yellow, ylabel near ticks, line width=3pt] table[x=x0, y=y0, col sep=comma]{linmcts/linmctsExpResults/document_length_experiment/alpha_0.01--1000--pretrained--c3.csv};
        \addplot[blue, ylabel near ticks, line width=3pt] table[x=x0, y=y0, col sep=comma]{linmcts/linmctsExpResults/document_length_experiment/alpha_0.1--1000--pretrained--c3.csv};
        \addplot[red, ylabel near ticks, line width=3pt] table[x=x0, y=y0, col sep=comma]{linmcts/linmctsExpResults/document_length_experiment/alpha_1.0--1000--pretrained--c3.csv};
        \addplot[black, dotted, ylabel near ticks, line width=3pt] table[x=x0, y=y3, col sep=comma]{linmcts/linmctsExpResults/document_length_experiment/alpha_0.1--1000--pretrained--c3.csv};
    \end{axis}
\end{tikzpicture}
\vspace{10pt}

\noindent\textbf{Analysis}
Let's first note that there are 8674, 10490 and 5796 documents for each of the respective graphs.
The longer the document, the highest is its maximum performing summary.
It seems like a single value of $\alpha$ is still the way to go, no matter the document length.
Smaller documents are easier to solve for the MCTS, much likely due to the exponentially smaller tree to explore.
The convergence is not only better but also faster for smaller documents.
This implies that one should attempt to scale the number of samples made by the MCTS with the number of sentences in a document.
A linear scaling seems legitimate, starting at 200 samples for documents of 10 sentences and ending at 500 for documents of 50 sentences i.e. $n = \ceil{7.5|d|} + 125$.

\section*{Conclusion}

\begin{itemize}
    \item \textbf{Pre-training formulation seems to be adequate.} The number of 1000 warmup steps is to be fixed and the number of sampled summaries per document should be tried at the lower value of 16.
    \item \textbf{Should explore values of $\alpha \in [0.05, 0.5]$}.
    \item Should try scaling number of MCTS samples per document with document length via $n = \ceil{7.5|d|} + 125$.
\end{itemize}

\section{Intégration}

Le linUCT génère des targets $\theta$, qu'on apprend par perte cosine.
À l'inférence, on utilise la linéarité du produit vectoriel
et le fait que nos représentations de résumés sont la somme
des représentations des phrases pour prédire les 3 phrases
dont la représentation a le produit vectoriel maximal avec la mapping
prédit.