@misc{2020t5,
      title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}, 
      author={Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
      year={2020},
      eprint={1910.10683},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{unilm,
 author = {Dong, Li and Yang, Nan and Wang, Wenhui and Wei, Furu and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {13063--13075},
 publisher = {Curran Associates, Inc.},
 title = {Unified Language Model Pre-training for Natural Language Understanding and Generation},
 url = {https://proceedings.neurips.cc/paper/2019/file/c20bb2d9a50d5ac1f713f8b34d9aac5a-Paper.pdf},
 volume = {32},
 year = {2019}
}


@inproceedings{zhang2019pegasus,
  title={Pegasus: Pre-training with extracted gap-sentences for abstractive summarization},
  author={Zhang, Jingqing and Zhao, Yao and Saleh, Mohammad and Liu, Peter},
  booktitle={International Conference on Machine Learning},
  pages={11328--11339},
  year={2020},
  organization={PMLR}
}

@inproceedings{dong2018banditsum,
    title = "{B}andit{S}um: Extractive Summarization as a Contextual Bandit",
    author = "Dong, Yue  and
      Shen, Yikang  and
      Crawford, Eric  and
      van Hoof, Herke  and
      Cheung, Jackie Chi Kit",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1409",
    doi = "10.18653/v1/D18-1409",
    pages = "3739--3748",
    abstract = "In this work, we propose a novel method for training neural networks to perform single-document extractive summarization without heuristically-generated extractive labels. We call our approach BanditSum as it treats extractive summarization as a contextual bandit (CB) problem, where the model receives a document to summarize (the context), and chooses a sequence of sentences to include in the summary (the action). A policy gradient reinforcement learning algorithm is used to train the model to select sequences of sentences that maximize ROUGE score. We perform a series of experiments demonstrating that BanditSum is able to achieve ROUGE scores that are better than or comparable to the state-of-the-art for extractive summarization, and converges using significantly fewer update steps than competing approaches. In addition, we show empirically that BanditSum performs significantly better than competing approaches when good summary sentences appear late in the source document.",
}

@inproceedings{luo-etal-2019-reading,
    title = "Reading Like {HER}: Human Reading Inspired Extractive Summarization",
    author = "Luo, Ling  and
      Ao, Xiang  and
      Song, Yan  and
      Pan, Feiyang  and
      Yang, Min  and
      He, Qing",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1300",
    doi = "10.18653/v1/D19-1300",
    pages = "3033--3043",
    abstract = "In this work, we re-examine the problem of extractive text summarization for long documents. We observe that the process of extracting summarization of human can be divided into two stages: 1) a rough reading stage to look for sketched information, and 2) a subsequent careful reading stage to select key sentences to form the summary. By simulating such a two-stage process, we propose a novel approach for extractive summarization. We formulate the problem as a contextual-bandit problem and solve it with policy gradient. We adopt a convolutional neural network to encode gist of paragraphs for rough reading, and a decision making policy with an adapted termination mechanism for careful reading. Experiments on the CNN and DailyMail datasets show that our proposed method can provide high-quality summaries with varied length, and significantly outperform the state-of-the-art extractive methods in terms of ROUGE metrics.",
}

@inproceedings{zhong-etal-2020-extractive,
    title = "Extractive Summarization as Text Matching",
    author = "Zhong, Ming  and
      Liu, Pengfei  and
      Chen, Yiran  and
      Wang, Danqing  and
      Qiu, Xipeng  and
      Huang, Xuanjing",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.552",
    doi = "10.18653/v1/2020.acl-main.552",
    pages = "6197--6208",
    abstract = "This paper creates a paradigm shift with regard to the way we build neural extractive summarization systems. Instead of following the commonly used framework of extracting sentences individually and modeling the relationship between sentences, we formulate the extractive summarization task as a semantic text matching problem, in which a source document and candidate summaries will be (extracted from the original text) matched in a semantic space. Notably, this paradigm shift to semantic matching framework is well-grounded in our comprehensive analysis of the inherent gap between sentence-level and summary-level extractors based on the property of the dataset. Besides, even instantiating the framework with a simple form of a matching model, we have driven the state-of-the-art extractive result on CNN/DailyMail to a new level (44.41 in ROUGE-1). Experiments on the other five datasets also show the effectiveness of the matching framework. We believe the power of this matching-based summarization framework has not been fully exploited. To encourage more instantiations in the future, we have released our codes, processed dataset, as well as generated summaries in \url{https://github.com/maszhongming/MatchSum}.",
}

@inproceedings{lin-2004-rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W04-1013",
    pages = "74--81",
}

@inproceedings{DBLP:journals/corr/SeeLM17,
    title = "Get To The Point: Summarization with Pointer-Generator Networks",
    author = "See, Abigail  and
      Liu, Peter J.  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P17-1099",
    doi = "10.18653/v1/P17-1099",
    pages = "1073--1083",
    abstract = "Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.",
}

@inproceedings{10.5555/3020488.3020497,
author = {Coquelin, Pierre-Arnaud and Munos, R\'{e}mi},
title = {Bandit Algorithms for Tree Search},
year = {2007},
isbn = {0974903930},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Bandit based methods for tree search have recently gained popularity when applied to huge trees, e.g. in the game of go [6]. Their efficient exploration of the tree enables to return rapidly a good value, and improve precision if more time is provided. The UCT algorithm [8], a tree search method based on Upper Confidence Bounds (UCB) [2], is believed to adapt locally to the effective smoothness of the tree. However, we show that UCT is "over-optimistic" in some sense, leading to a worst-case regret that may be very poor. We propose alternative bandit algorithms for tree search. First, a modification of UCT using a confidence sequence that scales exponentially in the horizon depth is analyzed. We then consider Flat-UCB performed on the leaves and provide a finite regret bound with high probability. Then, we introduce and analyze a Bandit Algorithm for Smooth Trees (BAST) which takes into account actual smoothness of the rewards for performing efficient "cuts" of sub-optimal branches with high confidence. Finally, we present an incremental tree expansion which applies when the full tree is too big (possibly infinite) to be entirely represented and show that with high probability, only the optimal branches are indefinitely developed. We illustrate these methods on a global optimization problem of a continuous function, given noisy values.},
booktitle = {Proceedings of the Twenty-Third Conference on Uncertainty in Artificial Intelligence},
pages = {67â€“74},
numpages = {8},
location = {Vancouver, BC, Canada},
series = {UAI'07}
}

@inproceedings{7860440,
  title={Improved LinUCT and its evaluation on incremental random-feature tree},
  author={Mandai, Yusaku and Kaneko, Tomoyuki},
  booktitle={2016 IEEE Conference on Computational Intelligence and Games (CIG)},
  pages={1--8},
  year={2016},
  organization={IEEE}
}

@article{NIPS2013_9aa42b31,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  journal={Advances in neural information processing systems},
  volume={26},
  pages={3111--3119},
  year={2013}
}

@inproceedings{10.5555/3298483.3298681,
  title={Summarunner: A recurrent neural network based sequence model for extractive summarization of documents},
  author={Nallapati, Ramesh and Zhai, Feifei and Zhou, Bowen},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={31},
  year={2017}
}

@inproceedings{pennington2014glove,
  title={Glove: Global vectors for word representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={1532--1543},
  year={2014}
}

@article{DBLP:journals/corr/PaulusXS17,
  title={A deep reinforced model for abstractive summarization},
  author={Paulus, Romain and Xiong, Caiming and Socher, Richard},
  journal={arXiv preprint arXiv:1705.04304},
  year={2017}
}


@article{kuleshov2014algorithms,
  title={Algorithms for multi-armed bandit problems},
  author={Kuleshov, Volodymyr and Precup, Doina},
  journal={arXiv preprint arXiv:1402.6028},
  year={2014}
}

@inproceedings{10.5555/2832249.2832384,
  title={Portfolio choices with orthogonal bandit learning},
  author={Shen, Weiwei and Wang, Jun and Jiang, Yu-Gang and Zha, Hongyuan},
  booktitle={Twenty-fourth international joint conference on artificial intelligence},
  year={2015}
}

@article{xu2015empirical,
  title={Empirical evaluation of rectified activations in convolutional network},
  author={Xu, Bing and Wang, Naiyan and Chen, Tianqi and Li, Mu},
  journal={arXiv preprint arXiv:1505.00853},
  year={2015}
}

@article{10.1162/neco.1997.9.8.1735,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={229--256},
  year={1992},
  publisher={Springer}
}

@inproceedings{sutton1999policy,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay},
  booktitle={Advances in neural information processing systems},
  pages={1057--1063},
  year={2000}
}

@inproceedings{peyrard-2019-studying,
  title={Studying summarization evaluation metrics in the appropriate scoring range},
  author={Peyrard, Maxime},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={5093--5100},
  year={2019}
}

@article{ng-abrecht-2015-better,
  title={Better summarization evaluation with word embeddings for rouge},
  author={Ng, Jun-Ping and Abrecht, Viktoria},
  journal={arXiv preprint arXiv:1508.06034},
  year={2015}
}

@inproceedings{peyrard-etal-2017-learning,
  title={Learning to Score System Summaries for Better Content Selection Evaluation.},
  author={Peyrard, Maxime and Botschen, Teresa and Gurevych, Iryna},
  booktitle={Proceedings of the Workshop on New Frontiers in Summarization},
  pages={74--84},
  year={2017}
}

@article{ucb,
  title={Finite-time analysis of the multiarmed bandit problem},
  author={Auer, Peter and Cesa-Bianchi, Nicolo and Fischer, Paul},
  journal={Machine learning},
  volume={47},
  number={2-3},
  pages={235--256},
  year={2002},
  publisher={Springer}
}



@inproceedings{chu2011contextual,
  title={Contextual bandits with linear payoff functions},
  author={Chu, Wei and Li, Lihong and Reyzin, Lev and Schapire, Robert},
  booktitle={Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  pages={208--214},
  year={2011}
}

@inproceedings{tikhonov1963solution,
  title={On the solution of ill-posed problems and the method of regularization},
  author={Tikhonov, Andrei Nikolaevich},
  booktitle={Doklady Akademii Nauk},
  volume={151},
  pages={501--504},
  year={1963},
  organization={Russian Academy of Sciences}
}


@book{banditalgs,
  title={Bandit algorithms},
  author={Lattimore, Tor and Szepesv{\'a}ri, Csaba},
  year={2020},
  publisher={Cambridge University Press}
}

@inproceedings{Li_2010,
  title={A contextual-bandit approach to personalized news article recommendation},
  author={Li, Lihong and Chu, Wei and Langford, John and Schapire, Robert E},
  booktitle={Proceedings of the 19th international conference on World wide web},
  pages={661--670},
  year={2010}
}

@article{kingma2014method,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{alphago,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{abbasi2011improved,
  title={Improved algorithms for linear stochastic bandits},
  author={Abbasi-Yadkori, Yasin and P{\'a}l, D{\'a}vid and Szepesv{\'a}ri, Csaba},
  journal={Advances in neural information processing systems},
  volume={24},
  pages={2312--2320},
  year={2011}
}

@article{halko2010finding,
  title={Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions},
  author={Halko, Nathan and Martinsson, Per-Gunnar and Tropp, Joel A},
  journal={SIAM review},
  volume={53},
  number={2},
  pages={217--288},
  year={2011},
  publisher={SIAM}
}

@article{10.1145/291128.291131,
  title={A semidiscrete matrix decomposition for latent semantic indexing information retrieval},
  author={Kolda, Tamara G and O'leary, Dianne P},
  journal={ACM Transactions on Information Systems (TOIS)},
  volume={16},
  number={4},
  pages={322--346},
  year={1998},
  publisher={ACM New York, NY, USA}
}


@article{sherman1950adjustment,
  title={Adjustment of an inverse matrix corresponding to a change in one element of a given matrix},
  author={Sherman, Jack and Morrison, Winifred J},
  journal={The Annals of Mathematical Statistics},
  volume={21},
  number={1},
  pages={124--127},
  year={1950},
  publisher={JSTOR}
}

@article{schuster1997bidirectional,
  title={Bidirectional recurrent neural networks},
  author={Schuster, Mike and Paliwal, Kuldip K},
  journal={IEEE transactions on Signal Processing},
  volume={45},
  number={11},
  pages={2673--2681},
  year={1997},
  publisher={Ieee}
}

@book{10.5555/3086952,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
  volume={1},
  year={2016},
  publisher={MIT press Cambridge}
}

@article{LIU201711,
  title={A survey of deep neural network architectures and their applications},
  author={Liu, Weibo and Wang, Zidong and Liu, Xiaohui and Zeng, Nianyin and Liu, Yurong and Alsaadi, Fuad E},
  journal={Neurocomputing},
  volume={234},
  pages={11--26},
  year={2017},
  publisher={Elsevier}
}

@article{robbins1951stochastic,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The annals of mathematical statistics},
  pages={400--407},
  year={1951},
  publisher={JSTOR}
}

@article{wu2016googles,
  title={Google's neural machine translation system: Bridging the gap between human and machine translation},
  author={Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and others},
  journal={arXiv preprint arXiv:1609.08144},
  year={2016}
}

@inproceedings{xu2016show,
  title={Show, attend and tell: Neural image caption generation with visual attention},
  author={Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhudinov, Ruslan and Zemel, Rich and Bengio, Yoshua},
  booktitle={International conference on machine learning},
  pages={2048--2057},
  year={2015}
}

@article{joulin2016bag,
  title={Bag of tricks for efficient text classification},
  author={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
  journal={arXiv preprint arXiv:1607.01759},
  year={2016}
}


@article{almeida2019word,
  author    = {Felipe Almeida and
               Geraldo Xex{\'{e}}o},
  title     = {Word Embeddings: {A} Survey},
  journal   = {CoRR},
  volume    = {abs/1901.09069},
  year      = {2019},
  url       = {http://arxiv.org/abs/1901.09069},
  archivePrefix = {arXiv},
  eprint    = {1901.09069},
  timestamp = {Sat, 02 Feb 2019 16:56:00 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1901-09069.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{hermann2015teaching,
  title={Teaching machines to read and comprehend},
  author={Hermann, Karl Moritz and Kocisky, Tomas and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
  journal={Advances in neural information processing systems},
  volume={28},
  pages={1693--1701},
  year={2015}
}

@article{devlin-etal-2019-bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}


@article{dou2020gsum,
  title={GSum: A general framework for guided neural abstractive summarization},
  author={Dou, Zi-Yi and Liu, Pengfei and Hayashi, Hiroaki and Jiang, Zhengbao and Neubig, Graham},
  journal={arXiv preprint arXiv:2010.08014},
  year={2020}
}

@inproceedings{kryscinski2020evaluating,
  title={Evaluating the factual consistency of abstractive text summarization},
  author={Kryscinski, Wojciech and McCann, Bryan and Xiong, Caiming and Socher, Richard},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={9332--9346},
  year={2020}
}

@article{narayan-etal-2018-ranking,
  title={Ranking sentences for extractive summarization with reinforcement learning},
  author={Narayan, Shashi and Cohen, Shay B and Lapata, Mirella},
  journal={arXiv preprint arXiv:1802.08636},
  year={2018}
}

@article{liu2019text,
  title={Text summarization with pretrained encoders},
  author={Liu, Yang and Lapata, Mirella},
  journal={arXiv preprint arXiv:1908.08345},
  year={2019}
}

@article{Robbins:1952,
  title={Some aspects of the sequential design of experiments},
  author={Robbins, Herbert},
  journal={Bulletin of the American Mathematical Society},
  volume={58},
  number={5},
  pages={527--535},
  year={1952},
  publisher={Citeseer}
}

@article{10.5555/2969442.2969476,
  title={Combinatorial bandits revisited},
  author={Combes, Richard and Talebi Mazraeh Shahi, Mohammad Sadegh and Proutiere, Alexandre and others},
  journal={Advances in neural information processing systems},
  volume={28},
  pages={2116--2124},
  year={2015}
}

@article{lairobbins,
  title={Asymptotically efficient adaptive allocation rules},
  author={Lai, Tze Leung and Robbins, Herbert},
  journal={Advances in applied mathematics},
  volume={6},
  number={1},
  pages={4--22},
  year={1985},
  publisher={Academic Press}
}

@inproceedings{pmlr-v28-chen13a,
  title={Combinatorial multi-armed bandit: General framework and applications},
  author={Chen, Wei and Wang, Yajun and Yuan, Yang},
  booktitle={International Conference on Machine Learning},
  pages={151--159},
  year={2013}
}


@book{rubinstein2013cross,
  title={The cross-entropy method: a unified approach to combinatorial optimization, Monte-Carlo simulation and machine learning},
  author={Rubinstein, Reuven Y and Kroese, Dirk P},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@article{kryscinski-etal-2019-neural,
  title={Neural text summarization: A critical evaluation},
  author={Kry{\'s}ci{\'n}ski, Wojciech and Keskar, Nitish Shirish and McCann, Bryan and Xiong, Caiming and Socher, Richard},
  journal={arXiv preprint arXiv:1908.08960},
  year={2019}
}


@inproceedings{pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  booktitle={Advances in neural information processing systems},
  pages={8026--8037},
  year={2019}
}

@inproceedings{tensorflow2015-whitepaper,
  title={Tensorflow: A system for large-scale machine learning},
  author={Abadi, Mart{\'\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
  booktitle={12th $\{$USENIX$\}$ symposium on operating systems design and implementation ($\{$OSDI$\}$ 16)},
  pages={265--283},
  year={2016}
}

@book{Sutton1998,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@inproceedings{soare2014multi,
  title={Multi-task linear bandits},
  author={Soare, Marta and Alsharif, Ouais and Lazaric, Alessandro and Pineau, Joelle},
  booktitle={NIPS2014 Workshop on Transfer and Multi-task Learning: Theory meets Practice},
  year={2014}
}

@inproceedings{deshmukh2017multi,
  title={Multi-task learning for contextual bandits},
  author={Deshmukh, Aniket Anand and Dogan, Urun and Scott, Clay},
  booktitle={Advances in neural information processing systems},
  pages={4848--4856},
  year={2017}
}


@article{contextual_bandits,
  title={Using confidence bounds for exploitation-exploration trade-offs},
  author={Auer, Peter},
  journal={Journal of Machine Learning Research},
  volume={3},
  number={Nov},
  pages={397--422},
  year={2002}
}

@inproceedings{NEURIPS2018_207f8801,
  title={Contextual combinatorial multi-armed bandits with volatile arms and submodular reward},
  author={Chen, Lixing and Xu, Jie and Lu, Zhuo},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3247--3256},
  year={2018}
}
