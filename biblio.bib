@article{2020t5,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}

@inproceedings{unilm,
  title     = {Unified Language Model Pre-training for Natural Language Understanding and Generation},
  author    = {Dong, Li and Yang, Nan and Wang, Wenhui and Wei, Furu and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
  year      = {2019},
  booktitle = {33rd Conference on Neural Information Processing Systems (NeurIPS 2019)}
}

@InProceedings{zhang2019pegasus, 
title = {{PEGASUS}: Pre-training with Extracted Gap-sentences for Abstractive Summarization}, 
author = {Zhang, Jingqing and Zhao, Yao and Saleh, Mohammad and Liu, Peter}, 
booktitle = {Proceedings of the 37th International Conference on Machine Learning}, 
year = {2020}, 
editor = {Hal Daum√© III and Aarti Singh}, 
volume = {119}, 
series = {Proceedings of Machine Learning Research}, 
address = {Virtual}, 
month = {13--18 Jul}, 
publisher = {PMLR}, 
pdf = {http://proceedings.mlr.press/v119/zhang20ae/zhang20ae.pdf}, 
url = {http://proceedings.mlr.press/v119/zhang20ae.html}
} 

@inproceedings{dong2018banditsum,
  title     = {BanditSum: Extractive Summarization as a Contextual Bandit},
  author    = {Dong, Yue and Shen, Yikang and Crawford, Eric and van Hoof, Herke and Cheung, Jackie Chi Kit},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  year      = {2018}
}

@inproceedings{luo-etal-2019-reading,
  title     = {Reading Like {HER}: Human Reading Inspired Extractive Summarization},
  author    = {Luo, Ling  and
      Ao, Xiang  and
      Song, Yan  and
      Pan, Feiyang  and
      Yang, Min  and
      He, Qing},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  month     = nov,
  year      = {2019},
  address   = {Hong Kong, China},
  publisher = {Association for Computational Linguistics},
  url       = {https://www.aclweb.org/anthology/D19-1300},
  doi       = {10.18653/v1/D19-1300},
  abstract  = {In this work, we re-examine the problem of extractive text summarization for long documents. We observe that the process of extracting summarization of human can be divided into two stages: 1) a rough reading stage to look for sketched information, and 2) a subsequent careful reading stage to select key sentences to form the summary. By simulating such a two-stage process, we propose a novel approach for extractive summarization. We formulate the problem as a contextual-bandit problem and solve it with policy gradient. We adopt a convolutional neural network to encode gist of paragraphs for rough reading, and a decision making policy with an adapted termination mechanism for careful reading. Experiments on the CNN and DailyMail datasets show that our proposed method can provide high-quality summaries with varied length, and significantly outperform the state-of-the-art extractive methods in terms of ROUGE metrics.}
}

@inproceedings{zhong-etal-2020-extractive,
  title     = {Extractive Summarization as Text Matching},
  author    = {Zhong, Ming  and
      Liu, Pengfei  and
      Chen, Yiran  and
      Wang, Danqing  and
      Qiu, Xipeng  and
      Huang, Xuanjing},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  month     = jul,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://www.aclweb.org/anthology/2020.acl-main.552},
  doi       = {10.18653/v1/2020.acl-main.552},
  abstract  = {This paper creates a paradigm shift with regard to the way we build neural extractive summarization systems. Instead of following the commonly used framework of extracting sentences individually and modeling the relationship between sentences, we formulate the extractive summarization task as a semantic text matching problem, in which a source document and candidate summaries will be (extracted from the original text) matched in a semantic space. Notably, this paradigm shift to semantic matching framework is well-grounded in our comprehensive analysis of the inherent gap between sentence-level and summary-level extractors based on the property of the dataset. Besides, even instantiating the framework with a simple form of a matching model, we have driven the state-of-the-art extractive result on CNN/DailyMail to a new level (44.41 in ROUGE-1). Experiments on the other five datasets also show the effectiveness of the matching framework. We believe the power of this matching-based summarization framework has not been fully exploited. To encourage more instantiations in the future, we have released our codes, processed dataset, as well as generated summaries in \url{https://github.com/maszhongming/MatchSum}.}
}

@inproceedings{lin-2004-rouge,
  title     = {{ROUGE}: A Package for Automatic Evaluation of Summaries},
  author    = {Lin, Chin-Yew},
  booktitle = {Text Summarization Branches Out},
  month     = jul,
  year      = {2004},
  address   = {Barcelona, Spain},
  publisher = {Association for Computational Linguistics},
  url       = {https://www.aclweb.org/anthology/W04-1013},
}

@article{DBLP:journals/corr/SeeLM17,
  author        = {Abigail See and
               Peter J. Liu and
               Christopher D. Manning},
  title         = {Get To The Point: Summarization with Pointer-Generator Networks},
  journal       = {CoRR},
  volume        = {abs/1704.04368},
  year          = {2017},
  url           = {http://arxiv.org/abs/1704.04368},
  archiveprefix = {arXiv},
  eprint        = {1704.04368},
  timestamp     = {Mon, 13 Aug 2018 16:46:08 +0200},
  biburl        = {https://dblp.org/rec/bib/journals/corr/SeeLM17},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{10.5555/3020488.3020497,
  author    = {Coquelin, Pierre-Arnaud and Munos, R\'{e}mi},
  title     = {Bandit Algorithms for Tree Search},
  year      = {2007},
  isbn      = {0974903930},
  publisher = {AUAI Press},
  address   = {Arlington, Virginia, USA},
  abstract  = {Bandit based methods for tree search have recently gained popularity when applied to huge trees, e.g. in the game of go [6]. Their efficient exploration of the tree enables to return rapidly a good value, and improve precision if more time is provided. The UCT algorithm [8], a tree search method based on Upper Confidence Bounds (UCB) [2], is believed to adapt locally to the effective smoothness of the tree. However, we show that UCT is "over-optimistic" in some sense, leading to a worst-case regret that may be very poor. We propose alternative bandit algorithms for tree search. First, a modification of UCT using a confidence sequence that scales exponentially in the horizon depth is analyzed. We then consider Flat-UCB performed on the leaves and provide a finite regret bound with high probability. Then, we introduce and analyze a Bandit Algorithm for Smooth Trees (BAST) which takes into account actual smoothness of the rewards for performing efficient "cuts" of sub-optimal branches with high confidence. Finally, we present an incremental tree expansion which applies when the full tree is too big (possibly infinite) to be entirely represented and show that with high probability, only the optimal branches are indefinitely developed. We illustrate these methods on a global optimization problem of a continuous function, given noisy values.},
  booktitle = {Proceedings of the Twenty-Third Conference on Uncertainty in Artificial Intelligence},
  location  = {Vancouver, BC, Canada},
  series    = {UAI'07}
}

@inproceedings{7860440,
  author    = {Y. {Mandai} and T. {Kaneko}},
  booktitle = {2016 IEEE Conference on Computational Intelligence and Games (CIG)},
  title     = {Improved LinUCT and its evaluation on incremental random-feature tree},
  year      = {2016},
  volume    = {},
  number    = {},
  doi       = {10.1109/CIG.2016.7860440}
}

@article{doi:10.1080/00029890.1969.12000364,
  author    = {J. F. Ramaley},
  title     = {Buffon's Noodle Problem},
  journal   = {The American Mathematical Monthly},
  volume    = {76},
  number    = {8},
  year      = {1969},
  publisher = {Taylor & Francis},
  doi       = {10.1080/00029890.1969.12000364},
  url       = { 
        https://doi.org/10.1080/00029890.1969.12000364
    
},
  eprint    = { 
        https://doi.org/10.1080/00029890.1969.12000364
    
}
}

@inproceedings{NIPS2013_9aa42b31,
  author    = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
  publisher = {Curran Associates, Inc.},
  title     = {Distributed Representations of Words and Phrases and their Compositionality},
  url       = {https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf},
  volume    = {26},
  year      = {2013}
}

@inproceedings{10.5555/3298483.3298681,
  author    = {Nallapati, Ramesh and Zhai, Feifei and Zhou, Bowen},
  title     = {SummaRuNNer: A Recurrent Neural Network Based Sequence Model for Extractive Summarization of Documents},
  year      = {2017},
  publisher = {AAAI Press},
  abstract  = {We present SummaRuNNer, a Recurrent Neural Network (RNN) based sequence model for extractive summarization of documents and show that it achieves performance better than or comparable to state-of-the-art. Our model has the additional advantage of being very interpretable, since it allows visualization of its predictions broken up by abstract features such as information content, salience and novelty. Another novel contribution of our work is abstractive training of our extractive model that can train on human generated reference summaries alone, eliminating the need for sentence-level extractive labels.},
  booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
  location  = {San Francisco, California, USA},
  series    = {AAAI'17}
}

@inproceedings{pennington2014glove,
  added-at  = {2016-02-18T12:02:38.000+0100},
  author    = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
  biburl    = {https://www.bibsonomy.org/bibtex/2a6e77a38c13e374ab250e13ae22993ec/thoni},
  booktitle = {EMNLP},
  interhash = {29813227df1eea94efa14c7df2b5553a},
  intrahash = {a6e77a38c13e374ab250e13ae22993ec},
  keywords  = {deeplearning deepwiki glove semantic},
  timestamp = {2016-09-06T08:23:07.000+0200},
  title     = {Glove: Global Vectors for Word Representation.},
  volume    = 14,
  year      = 2014
}

@article{DBLP:journals/corr/PaulusXS17,
  author        = {Romain Paulus and
               Caiming Xiong and
               Richard Socher},
  title         = {A Deep Reinforced Model for Abstractive Summarization},
  journal       = {CoRR},
  volume        = {abs/1705.04304},
  year          = {2017},
  url           = {http://arxiv.org/abs/1705.04304},
  archiveprefix = {arXiv},
  eprint        = {1705.04304},
  timestamp     = {Mon, 13 Aug 2018 16:48:58 +0200},
  biburl        = {https://dblp.org/rec/journals/corr/PaulusXS17.bib},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@article{kuleshov2014algorithms,
author = {Kuleshov, Volodymyr and Precup, Doina},
year = {2014},
month = {02},
pages = {},
title = {Algorithms for multi-armed bandit problems},
volume = {1},
journal = {Journal of Machine Learning Research}
}

@inproceedings{10.5555/2832249.2832384,
  author    = {Shen, Weiwei and Wang, Jun and Jiang, Yu-Gang and Zha, Hongyuan},
  title     = {Portfolio Choices with Orthogonal Bandit Learning},
  year      = {2015},
  isbn      = {9781577357384},
  publisher = {AAAI Press},
  abstract  = {The investigation and development of new methods from diverse perspectives to shed light on portfolio choice problems has never stagnated in financial research. Recently, multi-armed bandits have drawn intensive attention in various machine learning applications in online settings. The tradeoff between exploration and exploitation to maximize rewards in bandit algorithms naturally establishes a connection to portfolio choice problems. In this paper, we present a bandit algorithm for conducting online portfolio choices by effectually exploiting correlations among multiple arms. Through constructing orthogonal portfolios from multiple assets and integrating with the upper confidence bound bandit framework, we derive the optimal portfolio strategy that represents the combination of passive and active investments according to a risk-adjusted reward function. Compared with oft-quoted trading strategies in finance and machine learning fields across representative real-world market datasets, the proposed algorithm demonstrates superiority in both risk-adjusted return and cumulative wealth.},
  booktitle = {Proceedings of the 24th International Conference on Artificial Intelligence},
  location  = {Buenos Aires, Argentina},
  series    = {IJCAI'15}
}

@misc{xu2015empirical,
  abstract             = {{In this paper we investigate the performance of different types of rectified
activation functions in convolutional neural network: standard rectified linear
unit (ReLU), leaky rectified linear unit (Leaky ReLU), parametric rectified
linear unit (PReLU) and a new randomized leaky rectified linear units (RReLU).
We evaluate these activation function on standard image classification task.
Our experiments suggest that incorporating a non-zero slope for negative part
in rectified activation units could consistently improve the results. Thus our
findings are negative on the common belief that sparsity is the key of good
performance in ReLU. Moreover, on small scale dataset, using deterministic
negative slope or learning it are both prone to overfitting. They are not as
effective as using their randomized counterpart. By using RReLU, we achieved
75.68\% accuracy on CIFAR-100 test set without multiple test or ensemble.}},
  added-at             = {2017-07-19T15:29:59.000+0200},
  archiveprefix        = {arXiv},
  author               = {Xu, Bing and Wang, Naiyan and Chen, Tianqi and Li, Mu},
  biburl               = {https://www.bibsonomy.org/bibtex/2d15ce97ffbd56f6b67a3b3e9808d409f/andreashdez},
  citeulike-article-id = {13901176},
  citeulike-linkout-0  = {http://arxiv.org/abs/1505.00853},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1505.00853},
  day                  = 27,
  eprint               = {1505.00853},
  interhash            = {cd7f58f055c63e11c1555abb1e72b978},
  intrahash            = {d15ce97ffbd56f6b67a3b3e9808d409f},
  keywords             = {imported},
  month                = nov,
  posted-at            = {2016-04-29 20:51:58},
  priority             = {0},
  timestamp            = {2017-07-19T15:31:02.000+0200},
  title                = {{Empirical Evaluation of Rectified Activations in Convolutional Network}},
  url                  = {http://arxiv.org/abs/1505.00853},
  year                 = 2015
}




@article{iet:/content/conferences/10.1049/cp_19991218,
  author      = {F.A. Gers},
  affiliation = {IDSIA, Lugano},
  author      = {J. Schmidhuber},
  affiliation = {IDSIA, Lugano},
  author      = {F. Cummins},
  affiliation = {IDSIA, Lugano},
  keywords    = {learning;long short-term memory;recurrent neural networks;adaptive forget gate;resource allocation;},
  language    = {English},
  abstract    = {Long short-term memory (LSTM) can solve many tasks not solvable by previous learning algorithms for recurrent neural networks (RNNs). We identify a weakness of LSTM networks processing continual input streams without explicitly marked sequence ends. Without resets, the internal state values may grow indefinitely and eventually cause the network to break down. Our remedy is an adaptive ‚Äúforget gate‚Äù that enables an LSTM cell to learn to reset itself at appropriate times, thus releasing internal resources. We review an illustrative benchmark problem on which standard LSTM outperforms other RNN algorithms. All algorithms (including LSTM) fail to solve a continual version of that problem. LSTM with forget gates, however, easily solves it in an elegant way.},
  title       = {Learning to forget: continual prediction with LSTM},
  journal     = {IET Conference Proceedings},
  year        = {1999},
  month       = {January},
  publisher   = {Institution of Engineering and Technology},
  url         = {https://digital-library.theiet.org/content/conferences/10.1049/cp_19991218}
}

@article{williams1992simple,
  title     = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author    = {Williams, Ronald J},
  journal   = {Machine learning},
  volume    = {8},
  number    = {3-4},
  year      = {1992},
  publisher = {Springer}
}

@article{sutton1999policy,
  title   = {Policy gradient methods for reinforcement learning with function approximation},
  author  = {Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
  journal = {Advances in neural information processing systems},
  volume  = {12},
  year    = {1999}
}


@inproceedings{peyrard-2019-studying,
  title     = {Studying Summarization Evaluation Metrics in the Appropriate Scoring Range},
  author    = {Peyrard, Maxime},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  month     = jul,
  year      = {2019},
  address   = {Florence, Italy},
  publisher = {Association for Computational Linguistics},
  url       = {https://www.aclweb.org/anthology/P19-1502},
  doi       = {10.18653/v1/P19-1502},
  abstract  = {In summarization, automatic evaluation metrics are usually compared based on their ability to correlate with human judgments. Unfortunately, the few existing human judgment datasets have been created as by-products of the manual evaluations performed during the DUC/TAC shared tasks. However, modern systems are typically better than the best systems submitted at the time of these shared tasks. We show that, surprisingly, evaluation metrics which behave similarly on these datasets (average-scoring range) strongly disagree in the higher-scoring range in which current systems now operate. It is problematic because metrics disagree yet we can{'}t decide which one to trust. This is a call for collecting human judgments for high-scoring summaries as this would resolve the debate over which metrics to trust. This would also be greatly beneficial to further improve summarization systems and metrics alike.}
}

@inproceedings{ng-abrecht-2015-better,
  title     = {Better Summarization Evaluation with Word Embeddings for {ROUGE}},
  author    = {Ng, Jun-Ping  and
      Abrecht, Viktoria},
  booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
  month     = sep,
  year      = {2015},
  address   = {Lisbon, Portugal},
  publisher = {Association for Computational Linguistics},
  url       = {https://www.aclweb.org/anthology/D15-1222},
  doi       = {10.18653/v1/D15-1222},
  }

@article{61115,
  author  = {J. {Lin}},
  journal = {IEEE Transactions on Information Theory},
  title   = {Divergence measures based on the Shannon entropy},
  year    = {1991},
  volume  = {37},
  number  = {1},
  doi     = {10.1109/18.61115}
}

@inproceedings{peyrard-etal-2017-learning,
  title     = {Learning to Score System Summaries for Better Content Selection Evaluation.},
  author    = {Peyrard, Maxime  and
      Botschen, Teresa  and
      Gurevych, Iryna},
  booktitle = {Proceedings of the Workshop on New Frontiers in Summarization},
  month     = sep,
  year      = {2017},
  address   = {Copenhagen, Denmark},
  publisher = {Association for Computational Linguistics},
  url       = {https://www.aclweb.org/anthology/W17-4510},
  doi       = {10.18653/v1/W17-4510},
  abstract  = {The evaluation of summaries is a challenging but crucial task of the summarization field. In this work, we propose to learn an automatic scoring metric based on the human judgements available as part of classical summarization datasets like TAC-2008 and TAC-2009. Any existing automatic scoring metrics can be included as features, the model learns the combination exhibiting the best correlation with human judgments. The reliability of the new metric is tested in a further manual evaluation where we ask humans to evaluate summaries covering the whole scoring spectrum of the metric. We release the trained metric as an open-source tool.}
}

@article{ucb,
  title     = {Finite-time analysis of the multiarmed bandit problem},
  author    = {Auer, Peter and Cesa-Bianchi, Nicolo and Fischer, Paul},
  journal   = {Machine learning},
  volume    = {47},
  number    = {2-3},
  year      = {2002},
  publisher = {Springer}
}


@inproceedings{chu2011contextual,
  title     = {Contextual bandits with linear payoff functions},
  author    = {Chu, Wei and Li, Lihong and Reyzin, Lev and Schapire, Robert},
  booktitle = {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  year      = {2011}
}


@inproceedings{tikhonov1963solution,
  title        = {On the solution of ill-posed problems and the method of regularization},
  author       = {Tikhonov, Andrei Nikolaevich},
  booktitle    = {Doklady Akademii Nauk},
  volume       = {151},
  number       = {3},
  year         = {1963},
  organization = {Russian Academy of Sciences}
}

@book{banditalgs,
  place     = {Cambridge},
  title     = {Bandit Algorithms},
  doi       = {10.1017/9781108571401},
  publisher = {Cambridge University Press},
  author    = {Lattimore, Tor and Szepesv√°ri, Csaba},
  year      = {2020}
}

@article{Li_2010,
  title     = {A contextual-bandit approach to personalized news article recommendation},
  isbn      = {9781605587998},
  url       = {http://dx.doi.org/10.1145/1772690.1772758},
  doi       = {10.1145/1772690.1772758},
  journal   = {Proceedings of the 19th international conference on World wide web - WWW  ‚Äô10},
  publisher = {ACM Press},
  author    = {Li, Lihong and Chu, Wei and Langford, John and Schapire, Robert E.},
  year      = {2010}
}

@misc{kingma2014method,
  added-at = {2020-01-17T03:14:27.000+0100},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  biburl = {https://www.bibsonomy.org/bibtex/2d53bcfff0fe1a1d3a4a171352ee6e92c/simon_diener},
  description = {An upgrade over the standard stochastic gradient descend as it is able to apply changes to the learning rate by itself to be able to escape local maxima etc. This methods was used for the dynamic explainable recommender framework by Chen et al.},
  interhash = {57d2ac873f398f21bb94790081e80394},
  intrahash = {d53bcfff0fe1a1d3a4a171352ee6e92c},
  keywords = {final thema:neural_attentional_rating_regression},
  note = {cite arxiv:1412.6980Comment: Published as a conference paper at the 3rd International Conference  for Learning Representations, San Diego, 2015},
  timestamp = {2020-01-17T03:14:27.000+0100},
  title = {Adam: A Method for Stochastic Optimization},
  url = {http://arxiv.org/abs/1412.6980},
  year = 2014
}


@misc{agarap2018learning,
  added-at = {2018-03-23T15:12:05.000+0100},
  author = {Agarap, Abien Fred},
  biburl = {https://www.bibsonomy.org/bibtex/2ac4c868baa4ea267dad6d02b9c4b5c33/kylemccaulley},
  interhash = {0b73d7eb00fe360679c6741645b9c874},
  intrahash = {ac4c868baa4ea267dad6d02b9c4b5c33},
  keywords = {},
  timestamp = {2018-03-23T15:12:05.000+0100},
  title = {Deep Learning using Rectified Linear Units (ReLU)},
  url = {http://arxiv.org/abs/1803.08375},
  year = 2018
}

@article{alphago,
title	= {Mastering the game of Go with deep neural networks and tree search},
author	= {David Silver and Aja Huang and Christopher J. Maddison and Arthur Guez and Laurent Sifre and George van den Driessche and Julian Schrittwieser and Ioannis Antonoglou and Veda Panneershelvam and Marc Lanctot and Sander Dieleman and Dominik Grewe and John Nham and Nal Kalchbrenner and Ilya Sutskever and Timothy Lillicrap and Madeleine Leach and Koray Kavukcuoglu and Thore Graepel and Demis Hassabis},
year	= {2016},
URL	= {http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html},
journal	= {Nature},
volume	= {529}
}

@article{abbasi2011improved,
  title={Improved algorithms for linear stochastic bandits},
  author={Abbasi-Yadkori, Yasin and P{\'a}l, D{\'a}vid and Szepesv{\'a}ri, Csaba},
  journal={Advances in neural information processing systems},
  volume={24},
  year={2011}
}

@misc{halko2010finding,
      title={Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions}, 
      author={Nathan Halko and Per-Gunnar Martinsson and Joel A. Tropp},
      year={2010},
      eprint={0909.4061},
      archivePrefix={arXiv},
      primaryClass={math.NA}
}

@article{10.1145/291128.291131,
author = {Kolda, Tamara G. and O'Leary, Dianne P.},
title = {A Semidiscrete Matrix Decomposition for Latent Semantic Indexing Information Retrieval},
year = {1998},
issue_date = {Oct. 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/291128.291131},
doi = {10.1145/291128.291131},
abstract = {The vast amount of textual information available today is useless unless it can be effectively and efficiently searched. The goal in information retrieval is to find documents that are relevant to a given user query. We can represent and document collection by a matrix whose (i, j) entry is nonzero only if the ith term appears in the jth document; thus each document corresponds to a columm vector. The query is also represented as a column vector whose ith term is nonzero only if the ith term appears in the query. We score each document for relevancy by taking its inner product with the query. The highest-scoring documents are considered the most relevant. Unfortunately, this method does not necessarily retrieve all relevant documents because it is based on literal term matching.  Latent semantic indexing (LSI) replaces the document matrix with an approximation generated by the truncated singular-value decomposition (SVD). This method has been shown to overcome many difficulties associated with literal term matching. In this article we propose replacing the SVD with the semidiscrete decomposition (SDD). We will describe the SDD approximation, show how to compute it, and compare the SDD-based LSI method to the SVD-based LSI methods. We will show that SDD-based LSI does as well as SVD-based LSI in terms of document retrieval while requiring only one-twentieth the storage and one-half the time to compute each query. We will also show how to update the SDD approximation when documents are added or deleted from the document collection.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
pages = {322‚Äì346},
numpages = {25},
keywords = {semidiscrete decomposition, singular-value decomposition, data mining, latent semantic indexing, text retrieval}
}

@article{sherman1950adjustment,
  title={Adjustment of an inverse matrix corresponding to a change in one element of a given matrix},
  author={Sherman, Jack and Morrison, Winifred J},
  journal={The Annals of Mathematical Statistics},
  volume={21},
  number={1},
  pages={124--127},
  year={1950},
  publisher={JSTOR}
}

@article{schuster1997bidirectional,
  title={Bidirectional recurrent neural networks},
  author={Schuster, Mike and Paliwal, Kuldip K},
  journal={IEEE transactions on Signal Processing},
  volume={45},
  number={11},
  pages={2673--2681},
  year={1997},
  publisher={Ieee}
}
