\chapter{Concepts de base pertinents}     % numéroté
\label{chap:prerequis}                   % étiquette pour renvois (à compléter!)

Ce chapitre vise à présenter les concepts de base clés à la compréhension des travaux
présentés du reste de ce document.
Les concepts abordés ici sont accompagnés d'une explication aussi dénuée que possible 
de notions non-nécessaires aux travaux de ce mémoire, afin de garder leur présentation
digeste.
À cette fin, les concepts des problèmes de bandits, des réseaux de neurones et de la
génération automatique de résumés, tous essentiels à la compréhension de ce document,
sont donc présentés dans ce chapitre.

\section{Approches par bandit}
\label{sec:bandits}

On définit un bandit \citep{Robbins:1952} comme étant un problème où un 
apprenant interagit avec un environnement de manière séquentielle.
À chaque pas de temps $t$, l'apprenant sélectionne une action $a_t \in \mathcal{A}$ à effectuer 
parmi un ensemble $\mathcal{A}$ d'actions.
Pour chaque action $a_t$ sélectionnée, une récompense associée $X_{a_t,t}$
est observée.
Le but de l'apprenant est alors de maximiser les récompenses obtenues sur un horizon
fini d'interactions avec l'environnement.
Pour mesurer la performance d'un apprenant, on utilise une métrique appelée 
le regret cumulatif, qui mesure la différence de récompense entre l'action maximisant la récompense
sur l'environnement et les actions choisies par l'apprenant.
Formellement, les algorithmes qui s'attaquent à la résolution d'un problème de bandits 
ont l'objectif fondamental de minimiser le regret cumulatif.

L'objectif de minimisation du regret cumulatif fait intervenir un 
principe fondamental dans les problèmes où un apprenant interagit avec 
un environnement: le dilemme entre l'exploration et l'exploitation.
En effet, afin de minimiser le regret, un bandit doit limiter la sous-optimalité de
la récompense associée à l'action qu'il choisit.
Pour ce faire, il doit s'assurer que l'action sélectionnée à chaque 
pas de temps \textbf{exploite} les informations perçues par le passé 
mais \textbf{explore} aussi suffisamment les actions susceptibles 
d'être meilleures.

Notons que l'objectif de minimisation du regret cumulatif est différent des approches d'apprentissage automatique
supervisées: au lieu de s'intéresser seulement à la performance finale de
l'apprenant, on s'intéresse aussi à ce que sa performance s'améliore
rapidement.
En pratique, cet objectif est le plus approprié pour des problèmes
où il est crucial de ne pas commettre trop d'erreurs durant l'apprentissage.
Notamment, diverses déclinaisons des bandits ont été appliquées avec succès dans
les domaines des essais cliniques \citep{kuleshov2014algorithms}, de
la gestion de portfolios financiers \citep{10.5555/2832249.2832384} et
de la suggestion personnalisée d'articles de journaux \citep{Li_2010}.

On présente maintenant le bandit stochastique et sa déclinaison linéaire,
qui représentent les versions du problème bandit que l'on applique tout au long du document.
Pour chacune des versions, un algorithme permettant accomplissant la minimisation 
du regret associé aussi présenté.

\subsection{Bandit stochastique}
\label{subsec:bandit_stochastique}

Un bandit stochastique \citep{lairobbins} est un problème pour lequel on a un ensemble
$\mathcal{A}$ de $K$ actions disponibles.
% À chaque action $a \in \mathcal{A}$ est associée une récompense $X_{a,t}$ pour le $t$-ème 
% pas de temps effectué.
Dans le contexte stochastique, il est supposé que les récompenses $X_{a,t}$
d'une action sont indépendantes et identiquement distribuées (i.i.d.) 
selon une espérance fixe et inconnue de l'apprenant $\mu_a$, i.e. $\mu_a = \mathbb{E} \left[X_{a,t} \right]$.
Dans ce contexte, le but de l'apprenant est de maximiser l'\textbf{espérance} de ses récompenses.
On définit alors $a^* = \underset{a \in \mathcal{A}}{\argmax} \; \mu_a$
et $\mu^* = \mu_{a^*}$, respectivement l'action et l'espérance de récompense optimales.
Afin de mesurer la performance d'un apprenant sur un bandit stochastique, la métrique de performance
utilisée est le pseudo-regret cumulatif après $T$ interactions

\begin{equation}
    R_T = T \mu^* -  \sum_{t=1}^T  \mu_{a_t}.
    \label{eq:regret_cumulatif}
\end{equation}

Ici, $R_T$ représente la différence entre les récompenses attendues en sélectionnant
l'action optimale et les actions sélectionnées par l'apprenant.
Il s'agit donc d'une mesure de regret, mais appliquée sur l'espérance (inconnue)
de récompense de chaque action, d'où le nom pseudo-regret.

\subsection{Upper Confidence Bound (UCB)}
\label{subsec:ucb}

L'algorithme UCB \citep{ucb} est un algorithme de minimisation du pseudo-regret cumulatif \eqref{eq:regret_cumulatif}
pour les bandits stochastiques.
UCB est basé sur le principe de l'optimisme face à l'incertitude (OFUL), qui permet de
résoudre de manière élégante le dilemme exploration-exploitation décrit à la section 
\ref{sec:bandits}.
Pour suivre le principe OFUL, UCB maintient des bornes supérieures UCB$_a$ sur
l'espérance de chaque action $a$.
Les bornes UCB$_a$ sont construites de sorte à être supérieures à la valeur $\mu_a$ espérée
pour l'action avec une grande probabilité.
Après $t$ pas de temps effectués, la borne supérieure
pour l'espérance de l'action $a$ est définie selon

\begin{equation}
    \text{UCB}_a(t) = \bar{x}_a(t) + \beta \sqrt{\frac{2\ln t}{n_a(t)}},
    \label{eq:ucb}
\end{equation}

où $\beta$ est un hyperparamètre régulant l'exploration et 

\begin{equation*}
    \bar{x}_a (t) = \frac{1}{n_a (t)} \sum_{\tau = 1}^{t} \mathbb{I}\left[a_\tau = a\right] X_{a_\tau, \tau} \quad \text{et} \quad
    n_a (t) = \sum_{\tau = 1}^{t} \mathbb{I}\left[a_\tau = a\right]
\end{equation*}
représentent respectivement la moyenne des récompenses reçues jusqu'au temps $t$ pour l'action $a$ et
le nombre de fois que l'action $a$ a été sélectionnée.
Dans les équations de $\bar{x}_a (t)$ et $n_a(t)$, $\mathbb{I}$ est la fonction indicatrice retournant 1 quand 
son prédicat est vrai et 0 sinon.
Quand une action $a$ n'a pas encore été sélectionnée ($n_a(t)=0$), on 
lui assigne UCB$_a(t) = \infty$, forçant l'apprenant à sélectionner au moins 
une fois chaque action avant d'exploiter les moyennes $\bar{x}_a(t)$
dans son choix d'action.

La construction des bornes supérieures UCB$_a$ incorpore de manière explicite 
le dilemme entre l'exploration et l'exploitation.
En effet, le terme $\bar{x}_a(t)$ à gauche présente l'estimation actuelle de la récompense
associée à l'action $a$ et permet donc d'exploiter les connaissances acquises
dans les pas de temps précédant $t$.
À droite, le terme $\sqrt{\frac{2 \ln t}{n_a(t)}}$ est proportionnel à l'inverse
des sélections relatives.
Il s'agit donc d'un terme qui sera plus élevé pour les actions moins choisies,
incitant l'exploration d'actions sous-exploitées.
Ainsi, en sélectionnant l'action $a_t$ avec la borne supérieure maximale
selon les observations précédentes

\begin{equation*}
    a_t = \argmax_{a \in \mathcal{A}} \text{UCB}_a(t-1),
    \label{eq:ucb_selection}
\end{equation*}

l'algorithme UCB résout directement le dilemme exploitation-exploration.
Lorsque plusieurs actions ont la même valeur de borne supérieure, 
les égalités sont brisées de manière aléatoire.

\subsection{Bandit stochastique linéaire}
\label{subsec:bandit_lineaire}

La formulation en bandit stochastique linéaire reprend exactement le même
cadre formel que le bandit stochastique.
Les seules nouvelles notions sont la présence de représentations vectorielles
$\tilde{a} \in \tilde{\mathcal{A}} \subseteq \mathbb{R}^n$
pour les actions du problème et l'introduction de l'hypothèse linéaire.
Cette dernière suppose qu'il existe un vecteur de récompense unique $\omega_*$
permettant de relier chaque action $\tilde{a}$ à sa récompense espérée $\mu_a$, i.e. 
$\langle \omega_*, \tilde{a}\rangle \approx \mu_a$ pour tout $a$.
Sous cette formulation, le pseudo-regret cumulatif après $T$ interactions
devient

\begin{equation}
    R_T = T \langle \omega_*, \tilde{a}^*\rangle -  \sum_{t=1}^T  \langle \omega_*,\tilde{a}_t\rangle =\sum_{t=1}^T  \langle \omega_*, \tilde{a}^* - \tilde{a}_t\rangle,
    \label{eq:regret_cumulatif_lineaire}
\end{equation}

pour $\tilde{a}^*$, l'action avec l'espérance $\mu_a$ la plus élevée, dite action optimale.

\subsection{Linear Upper Confidence Bound (LinUCB)}
\label{subsec:pr_linucb}

L'algorithme LinUCB \citep{chu2011contextual} représente une application 
du principe de l'optimisme face à l'incertitude (OFUL) sur le problème de bandit stochastique linéaire
pour la minimisation du pseudo-regret cumulatif \eqref{eq:regret_cumulatif_lineaire}.

Rappelons que le principe OFUL consiste à avoir, pour chaque action $a$, un estimé optimiste UCB$_a$
de sa valeur espérée $\mu_a$, lequel est plus élevé que $\mu_a$ avec forte probabilité.
Dans le contexte linéaire, on cherche à avoir UCB$_a$ qui fait usage des relations linéaires entre les 
actions pour accélérer l'apprentissage.
On présente plus bas l'intuition derrière la construction de bornes supérieures
UCB$_a$ en omettant certains détails techniques pour alléger la lecture.
Le lecteur plus curieux pourra se référer à \citet{chu2011contextual} ou \citet{abbasi2011improved}
pour des descriptions techniques complètes.

Une première intuition clé pour bâtir les bornes supérieures consiste à exploiter 
les observations faites avant le temps $t$ pour bâtir un estimé $\hat{\omega}_t$ de 
$\omega_*$.
Cet estimé devrait naturellement viser 
à combiner aussi bien que possible les paires actions-récompenses $(\tilde{a}_\tau, X_\tau)$
observées, i.e.
\begin{equation}
    \langle \hat{\omega}_t, \tilde{a}_\tau\rangle \approx X_\tau, \quad \text{pour} \quad \tau \in \{1, ..., t\}.
    \label{eq:omega_n_assumption}
\end{equation}

On peut voir \eqref{eq:omega_n_assumption} comme un problème 
de moindres carrés, où l'on souhaite trouver la solution
\begin{equation}
    \hat{\omega}_t = \argmin_{\omega \in \mathbb{R}^n} \sum_{\tau=1}^{t} \left( \langle \omega, \tilde{a}_\tau \rangle - X_\tau \right)^2.
    \label{eq:lstsq}
\end{equation}

Or, le problème \eqref{eq:lstsq} n'admet pas nécessairement de solution exacte ou unique.
LinUCB emploie donc la régularisation de Tikhonov \citep{tikhonov1963solution} 
avec $\lambda=1$ pour garantir une solution et son unicité.
L'estimé $\hat{\omega}_t$ est alors choisi selon la solution analytique connue du
problème

\begin{equation}
    \hat{\omega}_t = \mathbf{V_t}^{-1}\mathbf{b_t}, \quad \text{pour} \quad \mathbf{V_t} = (\mathbf{A^\top_t} \mathbf{A_t} + \lambda \mathbf{I}) \quad \text{et} \quad \mathbf{b_t}=\mathbf{A_t^\top} \mathbf{X_t},
    \label{eq:omega_t}
\end{equation}

où $\mathbf{A_t}$ est la matrice dont les lignes sont les actions $\tilde{a}_\tau$ sélectionnées jusqu'à maintenant,
$\mathbf{X_t}$ est le vecteur composé des récompenses $X_\tau$ associées et $\mathbf{I}$ est la matrice identité.
En pratique, $\mathbf{V_t}$ et $\mathbf{b_t}$ peuvent tous deux être calculés de manière 
itérative selon 

\begin{equation}
\mathbf{V_t} = \lambda \mathbf{I} + \displaystyle \sum_{\tau=1}^{t} \tilde{a}_\tau \tilde{a}_\tau^\top \quad \text{et} \quad \mathbf{b_t} = \displaystyle \sum_{\tau=1}^{t} \tilde{a}_\tau X_\tau.
\label{eq:linucb_iteratif}
\end{equation}

Maintenant que l'on possède une bonne estimation $\hat{\omega}_t$ 
exploitant l'expérience récoltée, on cherche à introduire un terme garantissant une exploration 
satisfaisante des actions possibles.
LinUCB propose à cet effet d'utiliser 
\begin{equation}
    \beta\sqrt{\tilde{a}^\top \mathbf{V_t}^{-1} \tilde{a}},
    \label{eq:explo_linucb}
\end{equation}
un terme qui base l'exploration liée à une action $a$ à la différence de sa 
représentation vectorielle $\tilde{a}$ par rapport 
aux actions précédentes représentées par $\mathbf{V_t}$, que l'on pondère
encore une fois avec un hyperparamètre d'exploration $\beta$.
Les détails techniques complets permettant d'arriver au terme pour l'exploration sont 
donnés dans \citep{abbasi2011improved}.

À partir de \eqref{eq:omega_t} et \eqref{eq:explo_linucb}, on peut encore une fois 
bâtir une borne supérieure incorporant directement le compromis exploration-exploitation 

\begin{equation}
    \text{UCB}_a(t) = \langle \hat{\omega}_t^\top, \tilde{a} \rangle +  \beta\sqrt{\tilde{a}^\top \mathbf{V_t}^{-1} \tilde{a}},
    \label{eq:linucb}
\end{equation}

à partir de laquelle LinUCB choisit la borne supérieure maximale 

\begin{equation*}
    a_t = \argmax_{a \in \mathcal{A}} \text{UCB}_a(t-1).
    \label{eq:linucb_selection}
\end{equation*}



\subsubsection*{Connexions avec UCB}

On prend un moment ici pour mettre l'emphase sur les connexions entre UCB et 
LinUCB, qui peut être vu comme une généralisation linéaire directe de UCB
 \citep{banditalgs}. 
En effet, si l'on considère les vecteurs d'action correspondant aux vecteurs unitaires
orthogonaux $\tilde{a} = e_a$ et une régularisation de Tikhonov avec $\lambda = 0$, on 
a que $\mathbf{V_t}$ est une matrice diagonale, où l'entrée à la position $a$ est le
nombre $n_a(t)$ de sélections de l'action $a$.
À ce moment, le terme $e_a^\top \mathbf{V_t}^{-1} e_a$ n'est 
donc que $\frac{1}{n_a(t)}$ et le 
terme d'exploration devient
\begin{equation*}
    \beta\sqrt{\tilde{a}^\top \mathbf{V_t}^{-1} \tilde{a}} = \beta\sqrt{\frac{1}{n_a(t)}},
\end{equation*}
un terme proportionnel à celui utilisé dans UCB \eqref{eq:ucb}
si l'on pose $\beta' = \beta\sqrt{2\ln(t)}$.

Aussi, $\hat{\omega}_t$ devient simplement le vecteur où l'entrée $a$ correspond 
à la moyenne des récompenses perçues pour l'action $a$.
Le produit vectoriel $\langle \hat{\omega}_t, e_a \rangle$ ne fait donc qu'aller
chercher la moyenne $\bar{x}_a(t)$ utilisée dans UCB.
Ainsi, si l'on se positionne dans le cas où aucune information n'est partagée par 
les vecteurs d'action $\tilde{a}$, LinUCB est équivalent à UCB.
Comme cela correspond au pire cas pour LinUCB, celui où aucune relation linéaire 
n'est exploitable, LinUCB obtiendra toujours une performance supérieure ou égale à UCB
dans les cas où l'hypothèse linéaire est respectée.

\section{Réseaux de neurones}

L'apprentissage supervisé est un problème dans lequel on tente 
d'apprendre la relation entre des données en entrée 
et une valeur en sortie qui leur est associée.
Concrètement, on dit que l'on dispose d'un jeu de données 
$\mathcal{D} = \{(x_i, y_i) \}_{i=1}^N$ pour lequel on souhaite apprendre 
une fonction $f$ reliant chaque entrée à sa sortie, i.e. $f(x_i) \approx y_i$.
Cette formulation est générale et polyvalente: elle peut être appliquée 
à des tâches très variées allant de la classification d'images 
à la traduction de textes. 

En pratique, on résout habituellement un problème d'apprentissage 
supervisé en commençant par sélectionner une classe de fonctions paramétrées $\mathcal{F}$.
On tente alors de trouver les paramètres 
$\theta^*$ produisant l'approximateur paramétré $f_{\theta^*} \in \mathcal{F}$
reliant le mieux chaque entrée à sa sortie pour le problème.
Les réseaux de neurones (RN) \citep{10.5555/3086952} forment l'une de ces classes 
de fonctions paramétrées particulièrement intéressante. 
Leur utilisation a récemment explosé 
suite à leur excellente performance empirique et leur application 
à vraisemblablement n'importe quel problème supervisé \citep{LIU201711}.

Les RNs ne sont utilisés dans ce document
que pour permettre l'application des contributions.
On se limite donc à présenter le processus de 
descente de gradient que les RNs utilisent ainsi 
que les cas d'utilisation pertinents aux travaux.

\subsection{Descente de gradient}
\label{subsec:gradient_descent}

Pour trouver de bons paramètres $\theta$, les réseaux de neurones 
emploient une fonction de perte $l$ qui mesure la proximité entre 
une valeur prédite $\hat{y}=f_{\theta}(x)$ et la cible $y$ pour 
un exemple $(x,y)$ du jeu de données.
Hors mis la nécessité de représenter la proximité entre 
$\hat{y}$ et $y$, $l$ est seulement soumise 
à la contrainte d'être dérivable, i.e. le gradient $\nabla_{\hat{y}} l(\hat{y}, y)$ 
doit être bien défini.
Par exemple, pour des problèmes de classification binaire 
où les cibles $y$ et les valeurs prédites $\hat{y}$ sont des nombres entre 0 et 1,
une fonction de perte utilisable est l'entropie croisée binaire
\citep{rubinstein2013cross}

\begin{equation*}
    l(\hat{y}, y) = y \ln(\hat{y}) + (1 - y)\ln(1 - \hat{y}).
\end{equation*}

L'objectif de l'entraînement d'un RN est alors de trouver 
la paramétrisation $\theta^*$ minimisant la perte espérée, 
c'est à dire obtenir

\begin{equation}
    \theta^* = \argmin_{\theta} \mathcal{L}(\theta), \text{\quad où \quad} \mathcal{L}(\theta) =  \mathop{\mathbb{E}}_{(x, y) \sim \mathcal{D}} l\left(f_{\theta}(x), y\right).
    \label{eq:perte_esperee}
\end{equation}

On nomme $\mathcal{L}$ la perte espérée, en raison de l'espérance faite 
sur les exemples du jeu de données.
En pratique, avec n'importe quel jeu de données de taille raisonnable,
cette espérance ne peut toutefois pas être calculée efficacement et 
on utilise plutôt son estimation basée sur une \textit{minibatch} de 
$B$ exemples pigés aléatoirement

\begin{equation}
    \bar{\mathcal{L}}(\theta) =  \frac{1}{B} \displaystyle\sum_{i=1}^B l\left(f_{\theta}(x_i), y_i\right), \quad \text{pour} \quad \{(x_i, y_i)\}_{i=1}^B \sim \mathcal{D}^B,
    \label{eq:perte_stochastique}
\end{equation}

appelée la perte empirique.
Naturellement, plus la taille $B$ de la \textit{minibatch} est importante,
meilleure est l'estimation de la perte espérée et c'est pour cela 
que l'on choisit habituellement $B$ aussi grand que possible selon les
ressources de calcul à disposition.

Le problème d'optimisation \eqref{eq:perte_esperee} peut être résolu en
utilisant la perte empirique \eqref{eq:perte_stochastique}
en appliquant une descente de gradient.
En effet, comme on le verra bientôt, un réseau de neurones 
est entièrement dérivable par rapport à ses paramètres $\theta$ et 
on peut donc optimiser les paramètres selon la 
règle de mise à jour 

\begin{equation}
    \theta = \theta - \alpha \nabla \bar{\mathcal{L}}(\theta),
    \label{eq:gradient_descent}
\end{equation}

où $\alpha$ est un hyperparamètre appelé taux d'apprentissage.
En pratique, on utilise des optimiseurs, notamment SGD \citep{robbins1951stochastic}
et Adam \citep{kingma2014method}, qui implémentent des variantes 
de l'équation \eqref{eq:gradient_descent} pour trouver les paramètres $\theta^*$.


\subsection{Réseaux pleinement connectés}
\label{subsec:FC}

Un réseau pleinement connecté est la forme la plus simple 
de réseau de neurones.
Au niveau le plus fondamental, un réseau pleinement connecté
est composé d'un ensemble de couches $c_i$ consécutives.
Chaque couche est constituée d'une matrice $\mathbf{W}_i$ de poids 
et d'une fonction non-linéaire d'activation $\sigma_i$.
Pour un vecteur $h_i$ en entrée, une couche $c_i$ produit en sortie 
un autre vecteur $h_{i+1}$ qui sert d'entrée à la prochaine couche selon 

\begin{equation*}
    h_{i+1} = \sigma_i \left( \mathbf{W}_i h_i\right),
\end{equation*}

où $\mathbf{W}_i h_i$ est un produit matriciel et $\sigma_i$ 
est appliquée sur toutes les composantes du vecteur $\mathbf{W}_i h_i$.
Les seuls cas particuliers sont ceux de la première couche $c_1$, qui
prend le vecteur $h_1=x$ comme entrée, et de la dernière couche $c_n$ qui 
génère la valeur prédite $h_{n+1}=\hat{y}$.
Enfin, on définit les paramètres $\theta$ d'un RN comme 
l'ensemble des matrices $\mathbf{W}_i$ qui le composent.

Pour être admissible, une fonction d'activation $\sigma$ doit seulement 
être non-linéaire et dérivable.
Cela fait en sorte que les fonctions d'activation sont nombreuses 
dans la litérature.
Celles qui sont utilisées dans ce document sont:
\begin{itemize}
    \item ReLU \citep{xu2015empirical}: $\sigma(z) = \text{max}(0, z)$, dont la sortie 
    est toujours positive et non bornée.
    \item tanh: $\sigma(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$, dont la 
    sortie est entre -1 et 1.
    \item sigmoïde : $\sigma(z) = \frac{1}{1 + e^{-z}}$, dont la sortie est entre 
    0 et 1.
\end{itemize}

Le choix de la fonction d'activation utilisée à chaque couche dépend 
du comportement souhaité. 
On utilise habituellement les fonctions ReLU et tanh pour les
transitions entre deux couches consécutives alors que la fonction sigmoïde 
est habituellement employée en sortie quand les cibles sont entre 0 et 1.

\subsection{Réseaux récurrents}
\label{subsec:RNN}

Les réseaux pleinement connectés présentés à la section \ref{subsec:FC}
ne permettent pas d'exploiter la nature séquentielle de certains problèmes
comme ceux basés sur des données textuelles.
En effet, pour ces problèmes où l'entrée $x$ et la sortie $y$ associée sont 
des séquences $[x_1, ..., x_n]$ et $[y_1, ..., y_n]$, les réseaux 
pleinement connectés ne peuvent pas utiliser les potentielles 
relations entre les éléments d'une même séquence.
C'est à cet effet qu'ont été proposés les réseaux récurrents (RNN)
\citep{schuster1997bidirectional}.

Sous sa forme la plus simple, un RNN est représenté par trois matrices
de poids $\mathbf{U}$, $\mathbf{V}$ et $\mathbf{W}$ accompagnées de deux fonctions 
d'activation $\sigma_h$ et $\sigma_o$.
Pour chaque élément $x_i$ et représentation cachée $h_i$ en entrées, le RNN produit un vecteur 
en sortie $o_i$ et une représentation cachée $h_{i+1}$ qui sera utilisée pour 
le prochain élément selon 

\begin{equation*}
    h_{i+1} = \sigma_h\left(\mathbf{U}x_i + \mathbf{V}h_i\right) \text{\quad et \quad}
    o_i = \sigma_o\left(\mathbf{W}h_i\right).
\end{equation*}
L'idée d'un réseau récurrent est donc de générer une représentation cachée $h$
qui est utilisée et incrémentée à chaque nouvel élément $x_i$ de la séquence $x$ en entrée.

En pratique, les réseaux récurrents peuvent être bidirectionnels en incorporant 
trois nouvelles matrices $\mathbf{U}'$, $\mathbf{V}'$ et $\mathbf{W}'$ utilisées 
pour parcourir la séquence $x$ dans le sens contraire (de $x_n$ à $x_1$).
Il est aussi possible d'empiler plusieurs RNNs, en utilisant les sorties $o_i$
produites par un RNN comme séquence en entrée à un prochain RNN.
On dit alors que l'on a un réseau récurrent à $n$ couches, pour le nombre 
$n$ de RNNs empilés.
Une illustration contenant un réseau récurrent bidirectionnel à deux couches est disponible
à la figure \ref{fig:archi}.

Plus récemment, plusieurs des bons résultats empiriques obtenus avec 
des réseaux récurrents ont été obtenus en utilisant une variante plus 
complexe que celle présentée: le LSTM \citep{10.1162/neco.1997.9.8.1735}.
Les LSTMs incorporent explicitement dans leur architecture 
une représentation cachée plus riche 
permettant une meilleure circulation de l'information 
dans une séquence et un entraînement plus facile.
Ils ont notamment été appliqués avec succès sur la traduction 
de phrase \citep{wu2016googles} et la génération de description
d'image \citep{xu2016show}.

\subsection{Plongements de mots}

Un cas d'utilisation particulièrement intéressant 
des réseaux de neurones est celui de la génération de plongements 
de mots \citep{NIPS2013_9aa42b31,pennington2014glove,joulin2016bag}.
En entraînant un RN à prédire les mots dans une phrase,
il est possible d'obtenir des représentations vectorielles
de mots, communément appelés plongements de mots.

Les plongements incorporent des relations 
présentes entre les mots qu'ils représentent.
Par exemple, si l'on définit $p(m)$ comme le plongement 
associé au mot $m$, \citet{NIPS2013_9aa42b31} démontrent 
que la relation 
\begin{equation*}
    p(\text{”King”}) - p(\text{"Man"}) + p(\text{"Woman"}) = p(\text{"Queen"})
\end{equation*}
est respectée par les plongements de mots générés par leur technique.
La richesse des relations incorporées dans les plongements de mots 
est un des principaux facteurs de l'explosion récente de la performance des 
RNs sur les tâches de traitement de la langue naturelle $\mbox{\citep{almeida2019word}}$.

\section{Génération automatique de résumés}

Le sujet principal d'étude de ce document est la génération automatique 
de résumés de textes.
Formellement, un résumé est une version allégée d'un ou de plusieurs 
documents originaux.
Intuitivement, on souhaite qu'un résumé soit aussi court que 
possible mais qu'il conserve la majeure 
partie de l'information originale présente.

Pour notre part, on s'intéresse seulement au cas de la génération de résumés à partir
d'un seul document $d$ pour lequel on possède un seul résumé cible $s$.
Des approches existent spécifiquement pour la génération d'un résumé
à partir d'un ensemble de documents en entrée, mais on se contente
de dire que ca cas (multi-documents) se ramène grossièrement au cas de la génération d'un seul
document en considérant la concaténation de l'ensemble de documents.

Sous l'optique à laquelle on s'intéresse, la litérature peut actuellement être 
séparée en deux formulations distinctes: extractive et abstractive.
On présente chacune de ces méthodes, apportant une attention 
particulière à la formulation extractive qui fait l'objet 
d'une investigation détaillée dans ce document.
On en profite aussi pour bien définir comment il est possible 
d'évaluer la performance d'un système de génération de résumés 
en fin de section.

\subsection{Formulation extractive}
\label{sec:extractive}

Dans la formulation extractive, un résumé est généré en sélectionnant 
des phrases du document initial.
Cette formulation est intéressante car elle permet 
de (1) réduire drastiquement le nombre de résumés possibles, le faisant 
seulement dépendre du nombre de phrases d'un document, et (2) d'éviter 
toutes les difficultés en termes de syntaxe
et de cohérence liées à avoir à générer du texte de manière automatique.
Actuellement, les approches extractives de l'état de l'art sont
toutes basées sur l'obtention d'affinités pour chaque phrase, indiquant
si elle devrait être inclue ou non dans un résumé extractif.

Pour formaliser un peu les choses, on dit que l'on dispose de paires $(d,s)$,
représentant un document $d=[d_1, ..., d_{|d|}]$ de $|d|$ phrases
et son résumé cible $s$.
Un modèle de génération de résumés extractifs est un système à deux composantes 
$(\pi, \phi)$.
Ici, $\pi$ prend en entrée un document $d$ et retourne des affinités
$\pi(d) \in [0, 1]^{|d|}$ représentant à quel 
point chaque phrase devrait faire partie d'un résumé extractif.
On a ensuite $\phi$, qui est un processus de génération de résumé
extractif à partir des affinités $\pi(d)$.
Formellement, en définissant $\mathscr{P}(d)$ comme étant l'ensemble puissance 
des phrases de $d$ (i.e. $\mathscr{P}(d)$ contient tous les résumés extractifs possibles
de $d$), on a la signature $\phi(\pi(d)) \in \mathscr{P}(d)$.
Deux exemples intuitifs de $\phi$ sont les processus voraces et stochastiques,
où on choisit les $n$ phrases avec la plus grande probabilité ou on en pige
$n$ sans remise selon $\pi(d)$, respectivement.
Pour référence future, posons $\psi$ comme étant le processus vorace et
$\xi$ le processus stochastique.
Une illustration du processus général de génération de résumé extractif présenté
est disponible à la figure \ref{fig:summ}.


\tikzsetnextfilename{tikz_summ}
\begin{figure}[ht!]
    \begin{center}
        \begin{tikzpicture}[node distance=4.5cm, font=\large]
            \tikzstyle{default} = [rectangle, rounded corners, minimum width=4cm, minimum height=1cm, text centered, draw=black, line width=1pt, fill=gray!5, inner sep=0.25cm];
            \tikzstyle{sent} = [rectangle, minimum width=1.5cm, minimum height=1cm, text centered, draw=black, line width=1pt, fill=yellow!50, node distance=0.5cm];
            \tikzstyle{arrow} = [very thick,->,>=stealth, line width=1.5pt];
            
            \node (doc_t) {\textbf{Document} $d$};
            \node (d1) [sent, below=of doc_t] {$d_1$};
            \node (dn) [sent, below=of d1, yshift=-1cm] {$d_n$};
            \path (d1) -- (dn) node [font=\Huge, midway, sloped] {$\dots$};
            \begin{scope}[on background layer]
                \node (doc) [default, fit={(doc_t) (dn)}] {};
            \end{scope}
            
            \node (distro_t) [right=of doc_t]  {\textbf{Affinités} $\pi(d)$};
            \node (pi1) [node distance=0.5cm, minimum height=1cm, below=of distro_t] {\Large $\pi(d)_1$};
            \node (pin) [node distance=0.5cm, minimum height=1cm, below=of pi1, yshift=-1cm] {\Large $\pi(d)_n$};
            \path (pi1) -- (pin) node [font=\Huge, midway, sloped] {$\dots$};
            \begin{scope}[on background layer]
                \node (distro) [default, fit={(distro_t) (pin)}] {};
            \end{scope}
            
            \node (resume_t) [node distance=2cm, below=of distro] {\textbf{Résumé produit} $\hat{s}$};
            \node (di1) [sent, below=of resume_t] {$d_{i_1}$};
        \node (dik) [sent, below=of di1, yshift=-1cm] {$d_{i_k}$};
        \path (di1) -- (dik) node [font=\Huge, midway, sloped] {$\dots$};
        \begin{scope}[on background layer]
            \node (resume) [default, fit={(resume_t) (dik)}] {};
        \end{scope}
        
        \node (cible_t) [node distance=2cm, below=of doc] {\textbf{Résumé cible} $s$};
        \node (s1) [sent, fill=red!50, below=of cible_t] {$s_1$};
        \node (sm) [sent, fill=red!50, below=of s1, yshift=-1cm] {$s_m$};
        \path (s1) -- (sm) node [font=\Huge, midway, sloped] {$\dots$};
        \begin{scope}[on background layer]
            \node (cible) [default, fit={(cible_t) (sm)}] {};
        \end{scope}
        
        \coordinate (milieu) at ($(cible)!0.5!(resume)$);
        \node (eval_t) [below of = milieu, yshift=-2cm]  {\textbf{Évaluation de la qualité}};
        % \node (R) [node distance=0.5cm, minimum height=0.5cm, below=of eval_t] {\Large $\text{ROUGE}(\hat{s}, s)$};
        \begin{scope}[on background layer]
            \node (eval) [default, fit={(eval_t)}] {};
        \end{scope}
        
        \draw [arrow] (doc) -- node[anchor=south] {\Large $\pi_\theta$} (distro);
        \draw [arrow] (distro) -- node[anchor=west] {\Large $\phi$} (resume);
        \draw [arrow] (doc) -- node[anchor=west] {} (cible);
        
        \coordinate (milieu_bas) at ($(cible.south)!0.5!(resume.south)$);
        \coordinate (y) at ($(milieu_bas)!0.5!(eval.north)$);
        \draw [very thick,>=stealth, line width=1.5pt] (cible.south) -- (y);
        \draw [very thick,>=stealth, line width=1.5pt] (resume.south) -- (y);
        \draw [arrow] (y) -- (eval.north);
    \end{tikzpicture}
\end{center}
\caption[Processus de génération d'un résumé extractif]{Schéma des étapes de la génération de résumé extractif pour un document $d$ et 
son résumé cible $s$. La couleur différente utilisée pour les phrases du résumé 
cible sert à indiquer qu'elles ne sont habituellement pas des phrases du document
en entrée.}
\label{fig:summ}
\end{figure}

Enfin, pour un problème de génération de résumés donné, on fixe habituellement $\phi$.
L'objectif d'apprentissage est alors de trouver des paramètres $\theta$
qui permettent de maximiser la qualité des résumés produits par le modèle
$(\pi_\theta, \phi)$ à partir des paires document-résumé cibles $(d, s)$ contenues
dans un jeu de données $\mathcal{D}$.

\subsubsection*{Approches supervisées}

La majorité des approches extractives de l'état de l'art sont entraînées 
de manière supervisée à partir de cibles pour les affinités d'un document.
Or, comme le résumé $s$ correspondant à un document $d$ n'est habituellement pas obtenu de
manière extractive (i.e. le résumé n'est pas une combinaison de phrases du document initial),
il n'est pas trivial d'obtenir des cibles pour les affinités.
En pratique, les approches supervisées doivent utiliser des cibles basées sur des heuristiques pour
leur entraînement.

Par exemple, \citet{10.5555/3298483.3298681} précalculent des cibles binaires $y_d \in \{0,1\}^{|d|}$
utilisées comme cibles pour chaque document $d$.
Leurs cibles binaires représentent les phrases choisies par un oracle
sélectionnant de manière vorace les phrases $d_i \in d$ permettant de générer
un résumé extractif aussi près que possible du résumé cible $s$.
Après avoir sélectionné trois phrases, leur processus est arrêté et la cible $y_d$
d'un document $d$ est fixée à un vecteur binaire où les index des trois phrases 
retenues ont une valeur de 1 et les autres index ont une valeur nulle.

L'emploi de cibles binaires permet un entraînement supervisé très efficace:
\citet{liu2019text} utilisent seulement les cibles binaires
et représentent actuellement l'état de l'art extractif 
sur le populaire jeu de données du CNN/DailyMail \citep{hermann2015teaching}.
Notons toutefois qu'un réseau entraîné de manière supervisée à 
imiter un oracle ne peut pas obtenir une performance supérieure 
à celle de l'oracle.
Ainsi, les approches supervisées sont plus difficilement applicables
à des contextes où un bon oracle n'est pas disponible.

\subsubsection*{Approches par renforcement}
\label{subsec:rl_summ}

Au lieu d'utiliser des cibles binaires, les approches par renforcement visent à
optimiser directement une mesure numérique de la similarité entre deux résumés.
Disons que l'on dispose d'une métrique numérique de performance $G(\hat{s}, s)$
permettant de quantifier la proximité entre un résumé produit $\hat{s}$ et un résumé
cible $s$.
Il est alors possible de définir la fonction objective à maximiser
\begin{equation}
    J(\theta) = \underset{(d,s) \sim \mathcal{D}}{\mathbb{E}} \; \underset{\hat{s} \sim \phi(\pi_\theta(d))}{\mathbb{E}} \left[G(\hat{s}, s) \right],
    \label{eq:REINFORCE_expectation}
\end{equation}

représentant l'espérance de performance des résumés produits avec la paramétrisation $\theta$.
Comme on recherche $\theta^*$ maximisant $J(\theta)$, il vient naturellement en tête
de faire appel à des méthodes d'ascension de gradient\footnote{De manière analogue à
la descente de gradient présentée à la section \ref{subsec:gradient_descent} pour la minimisation
de fonction, l'ascension de gradient permet de maximiser une fonction.}.
Or, $J$ n'est pas calculable analytiquement en pratique car les deux espérances (sur
tous les documents et sur tous les résumés générables) sont habituellement intractables.

L'algorithme REINFORCE \citep{williams1992simple} propose une
solution élégante à ce problème, faisant une mise à jour des poids $\theta$ 
selon l'approximation du gradient de $J(\theta)$ basée sur un seul échantillon de $J$ 
\begin{equation}
    \nabla J(\theta) = G(\hat{s}, s)\nabla \ln \phi(\hat{s} | \pi_\theta, d), \quad d \sim \mathcal{D}, \hat{s} \sim \phi (\pi_\theta(d))
    \label{eq:REINFORCE_sample}
\end{equation}

où $\phi(\hat{s} | \pi_\theta, d)$ représente la probabilité de générer le résumé 
$\hat{s}$ à partir de $\pi_\theta(d)$ et $\phi$.


Le théorème du gradient de politique \citep{sutton1999policy} affirme que, en espérance,
l'approximation \eqref{eq:REINFORCE_sample} correspond au gradient de \eqref{eq:REINFORCE_expectation}
et peut donc être utilisée pour une ascension de gradient.
Les approches par renforcement basées sur REINFORCE \citep{dong2018banditsum,luo-etal-2019-reading}
parviennent actuellement à atteindre des performances très près de l'état
de l'art pour la génération automatique de résumés à un coût de calcul 
nettement inférieur.

\subsection{Formulation abstractive}

Sous cette formulation, on génère le résumé d'un texte un mot à la fois.
Les résumés abstractifs peuvent donc potentiellement reproduire 
de manière parfaite le résumé cible associé à un document.
Cette performance potentielle vient toutefois à un coût:
la formulation abstractive est naturellement plus difficile car elle nécessite de
gérer la syntaxe et les fautes d'orthographe en plus de la tâche 
de trouver les informations pertinentes du document.
De surcroît, les modèles abstractifs peuvent aussi être sujets à la génération 
d'énoncés factuellement incorrects \citep{kryscinski2020evaluating} .

Comme on s'intéresse aux méthodes extractives pour le reste du document,
on se contente de référer le lecteur aux approches abstractives
représentant l'état de l'art \citep{dou2020gsum, 2020t5, unilm, zhang2019pegasus}.
Au coeur de toutes ces approches se trouvent les architectures BERT 
\citep{devlin-etal-2019-bert} et Transformer \citep{vaswani2017attention},
auxquelles sont dues les percées récentes dans le domaine de la génération 
de texte.

\subsection{Évaluation de la performance}
\label{sec:rouge}

À la section \ref{sec:extractive}, on prenait pour acquis qu'une fonction $G$ existait
pour mesurer la proximité entre un résumé généré $\hat{s}$ et un résumé cible $s$.
Il en existe en fait plusieurs, notamment :

\begin{itemize}
    \item ROUGE-$n$ \citep{lin-2004-rouge}: métrique basée sur le chevauchement entre les séquences de $n$ mots ($n$-grammes)
          de $\hat{s}$ et $s$. Par exemple, $\text{ROUGE-1}$ représente le chevauchement entre les unigrammes
          et $\text{ROUGE-2}$ celui entre les bigrammes;
    \item ROUGE-L \citep{lin-2004-rouge}: métrique mesurant la plus longue sous-séquence commune
          entre $\hat{s}$ et;
    \item ROUGE-WE \citep{ng-abrecht-2015-better}: métrique similaire à ROUGE-$n$, mais qui utilise
          le \textit{soft-matching} basé sur la similarité entre les plongements de mots au lieu
          de la correspondance exacte.
\end{itemize}
L'intuition ici est simple: plus le chevauchement est grand entre les 
\ngrams d'un résumé généré et ceux du résumé cible, plus le résumé
généré a conservé l'information recherchée.

Or, si on se fie seulement au rappel sur les \textit{n}-grammes, le résumé optimal sera
de conserver tout le texte original. 
Pour pénaliser les résumés trop peu concis, on peut utiliser une métrique plus
appropriée que le rappel comme le score F1 pour pénaliser les 
\ngrams présents dans le résumé généré mais pas dans la cible.
Pour tout le document, tous les scores basés sur ROUGE rapportés seront donc 
basés sur la métrique F1.

Bien que ces métriques soient généralement
corrélées avec l'avis d'un expert humain, elles peuvent difficilement être considérées
comme un remplacement de celui-ci \citep{peyrard-2019-studying}.
En effet, comme les métriques sont toutes similairement corrélées avec le jugement humain
mais qu'elles ne sont que faiblement corrélées entre elles, aucune des métriques
ne peut être utilisée à elle seule comme un remplacement de l'avis humain.
En utilisant une moyenne de plusieurs métriques, il est possible d'alléger quelque peu
ce problème.
Notons toutefois que l'utilisation d'une moyenne ne suffit pas à régler le problème
en entier.
Par exemple, \citet{DBLP:journals/corr/PaulusXS17} entraînent 
plusieurs modèles abstractifs mais finissent par déployer un modèle n'atteignant
pas la meilleure moyenne de score ROUGE-1, ROUGE-2 et ROUGE-L après
une évaluation de la qualité des modèles par des humains.

Enfin, il est aussi important de mentionner que toutes les métriques énoncées 
plus haut ne sont pas dérivables.
Elles ne peuvent donc pas être utilisées directement comme fonctions de perte
pour l'entraînement d'un réseau de neurones.

\subsubsection*{Évaluation de la performance à partir d'un jeu de données}

On termine en mentionnant que, dans le contexte où l'on apprend à partir 
d'un jeu de données $\mathcal{D}=\{(d_i, s_i)\}_{i=1}^N$, il est important 
de faire l'évaluation sur un jeu de données différent de celui utilisé pour l'entraînement.
En effet, comme on s'intéresse à la performance d'un modèle de génération 
de résumés sur n'importe quel document en entrée, il est nécessaire de valider 
la performance du modèle sur des documents qui n'ont pas encore été vus par 
le modèle.

À cet effet, on sépare habituellement le jeu de données $\mathcal{D}$ en trois
sous-ensembles: un ensemble d'entraînement, un ensemble de validation et un ensemble 
de test.
L'ensemble d'entraînement est naturellement utilisé pour l'apprentissage du modèle
alors que l'ensemble de validation est utilisé pour estimer la performance 
sur des nouveaux documents pendant l'entraînement.
À la fin de l'entraînement, le modèle retenu est celui ayant la meilleure performance 
sur le jeu de validation.
Enfin, l'ensemble de test est utilisé pour estimer la performance réelle 
du modèle retenu. 
