\chapter{Concepts de base pertinents aux travaux}     % numéroté
\label{chap:prerequis}                   % étiquette pour renvois (à compléter!)

Ce chapitre vise à présenter les concepts de base clés à la compréhension des travaux
présentés dans le reste du document.
Les concepts abordés ici sont accompagnés d'une explication aussi dénuée de notions
non-nécessaires que possible aux travaux de ce mémoire, afin de garder leur présentation
aussi digeste que possible.
À cette fin, les concepts des algorithmes de bandits, des réseaux de neurones et de la
génération automatique de résumés, tous essentiels à la compréhension de ce document,
sont donc présentés dans ce chapitre.
À la fin de ce chapitre, le lecteur trouvera ainsi une présentation formelle du
problème exploré dans ce document, sous toutes les contraintes et hypothèses retenues.

\section{Approches de bandits}

Formulation générale Multi-Armed Bandit (MAB) stochastique: Problème pour lequel on a un ensemble
$\mathcal{A}$ de $k$ actions disponibles (bras).
À chaque action $a$ est associée une récompense $X_{a,t}$ pour la $t$-ème fois qu'elle
est choisie.
Les récompenses ne sont pas nécessairement identiques pour tout $t$, mais on suppose
qu'il existe $\mu_a$ tel que $\mu_a = \mathbb{E} \left[X_{a,t} \right]$.

On définit alors l'action optimale $a^* = \underset{a \in \mathcal{A}}{\argmax} \; \mu_a$.
Conjointement, on a aussi $\mu^* = \mu_{a^*}$, l'espérance du bras optimal.
L'objectif d'un algorithme visant à résoudre le MAB est de sélectionner
successivement des actions $a_t$ et de maximiser les récompenses $X_t$ perçues.
Pour normaliser la formulation à des récompenses d'amplitude variée, la métrique de performance
utilisée est le regret cumulatif

\begin{equation}
    \rho_n = n \mu^* -  \sum_{t=1}^n  X_t,
    \label{eq:regret_cumulatif}
\end{equation}

qui représente la différence entre la récompense qui aurait été reçue en prenant
l'action optimale et celle reçue par l'action sélectionnée.
Les algorithmes sont donc généralement formulés de sorte à minimiser le regret cumulatif.

Il est important de comprendre que l'objectif de minimisation du regret cumulatif
ne consiste pas exactement à trouver l'action optimale.
En effet, comme le regret cumulatif inclut les erreurs commises
depuis le début de l'apprentissage, un bon algorithme au sens de la minimisation
du regret cumulatif doit à la fois converger à une action presque optimale, mais il
doit aussi le faire en mettant de côté rapidement les actions les plus sous-optimales.

En pratique, la formulation MAB est donc la plus appropriée pour des problèmes
où il est crucial de ne pas commettre trop d'erreurs durant l'apprentissage.
Notamment, diverses déclinaisons des bandits ont été appliquées avec succès dans
les domaines des essais cliniques \citep{kuleshov2014algorithms} et de
la gestion de portfolios financiers \citep{10.5555/2832249.2832384}.

\section{Réseaux de neurones}

Classe de fonctions $h_\theta$ paramétrées par des poids $\theta \in \mathbb{R}^n$ qui peuvent être appliquées pour apprendre
virtuellement n'importe quelle fonction $f$ à l'étude.
Pour apprendre, les réseaux de neurones utilisent des approches de
descente de gradient sur une fonction de perte $\mathcal{L}(\theta)$.
Le fonction de perte, dont la forme dépend de celle de la fonction
à l'étude, sert de guide d'apprentissage au réseau.
Essentiellement, pour des paramètres $\theta$ et $\theta'$
pour lesquels on a $\mathcal{L}(\theta) < \mathcal{L}(\theta')$, on a
que les paramètres $\theta$ produisent une approximation $h_\theta$
plus près de la fonction cible $f$.
Éventuellement, l'apprentissage d'un réseau de neurones a pour objectif
ultime de trouver les paramètres optimaux
\begin{equation*}
    \theta^* = \underset{\theta \in \mathbb{R}^n}{\argmin} \; \mathcal{L}(\theta).
\end{equation*}

\subsection{Réseaux pleinement connectés}

Présentation haut niveau de leur utilité prouvée empiriquement
sur à peu près n'importe quel type de problème d'apprentissage
supervisé.

\begin{itemize}
    \item Unité de base: le neurone. Neurone prend entrée un vecteur de caractéristiques numériques.
          Le neurone fait une somme pondérée sur les caractéristiques en entrée et applique une
          fonction d'activation pour obtenir une couche de non-linéarité.
    \item Fonction d'activation: ReLU \citep{xu2015empirical}.
    \item Les neurones sont regroupées ensemble en couches, où un vecteur de caractéristiques
          est passé à travers une séquence de couches, où la sortie d'une couche représente l'entrée
          de la prochaine.
    \item $\theta$ est un ensemble de matrices, où chaque matrice représente les poids
          reliant le neurone $i$ d'une couche à la caractéristique $j$ en sortie de la couche précédente.
    \item Fonction de perte quadratique: permet une convergence rapide en pénalisant sévèrement
          les erreurs graves et peu les erreurs très faibles.
\end{itemize}

\subsection{Réseaux récurrents}

Cas où les données sont des séquences de tailles variables (i.e. données
textuelles).

Juste présenter le LSTM \citep{iet:/content/conferences/10.1049/cp_19991218}.
Architecture qui incorpore les notions de mémoire à court et à long termes pour
exploiter les relations séquentielles courtes et longues présentes dans une même
séquence en entrée.
Important à noter: pour une séquence $[x_1, ..., x_n]$ en entrée, un LSTM produit une
séquence $[\hat{y_1}, ..., \hat{y_n}]$ de vecteurs $\hat{y_i}$ pour chacun des éléments.

\subsection{Plongements de mots}
On peut utiliser des réseaux pour apprendre des représentations
vectorielles de mots \citep{NIPS2013_9aa42b31,pennington2014glove}.

Ces représentations vectorielles permettent d'exprimer certaines
relations linéaire entre les mots ("roi" - "homme" + "femme" = "reine").

Les plongements de mots sont à la base de l'explosion de performance dans les
tâches liées aux données textuelles grâce à leur utilisation avec des réseaux de
neurones.

\section{Génération automatique de résumés}

Quand on résume, on veut (1) compresser un ou des textes tout en
s'assurant de (2) conserver la majeure partie de l'information contenue.

On dit qu'il s'agit d'un dilemme compression-conservation.

Deux techniques principales actuellement utilisées: extractive et abstractive.

Nous nous intéresserons seulement au cas de la génération de résumés à partir
d'un seul document $d$ pour lequel nous possédons un seul résumé cible $s$.

Notons que le cas où on génère un résumé à partir de plusieurs documents se ramène
naturellement au cas d'un seul document en considérant un nouveau document
représentant la concaténation de tous les documents en entrée.

\subsection{Formulation extractive}
\label{sec:extractive}

On sélectionne des phrases du document initial.

Formulation simple à résoudre: la nombre de résumés dépend maintenant seulement du nombre
de phrases et on s'évite toute les difficultés en termes de syntaxe et de cohérence
d'avoir à générer du texte.

Les approches de l'état de l'art sont toutes basées sur une approche
encodeur-décodeur avec des réseaux de neurones.

Ces approches produisent en sortie une distribution sur les phrases
originales.

Le résumé est ensuite bâti à partir de la distribution prédite par le modèle.

Définition formelle: On a un document en entrée $d$ qui contient $|d|$ phrases $[d_1, ..., d_{|d|}]$
et pour lequel on a un résumé cible $s$.
On dit qu'on a un modèle de génération de résumés à deux composantes $(\pi, \phi)$.
Ici, $\pi$ prend en entrée un document $d$ et retourne
$\pi(d) \in [0, 1]^{|d|}$, une distribution de probabilités de sélection sur les phrases
de $d$.
On a ensuite $\phi$, qui est un processus de génération de résumé
extractif à partir d'une distribution $\pi(d)$.
Concrètement, en définissant $\mathscr{P}(d)$ comme étant le power set de l'ensemble
des phrases de $d$ (i.e. $\mathscr{P}(d)$ contient tous les résumés extractifs possibles
de $d$), on a la signature $\phi(\pi(d)) \in \mathscr{P}(d)$.
Deux exemples intuitifs de $\phi$ sont les processus voraces et stochastiques,
où on choisit les $n$ phrases avec la plus grande probabilité ou on en pige
$n$ sans remise de $\pi(d)$, respectivement.

Enfin, pour un problème de génération de résumés donnés, on fixe habituellement $\phi$
pour tous les documents.
L'objectif d'apprentissage est alors de trouver des paramètres $\theta$
qui permettent de maximiser la qualité des résumés produits par le modèle
$(\pi_\theta, \phi)$ par rapport à des paires document-résumé cible $(d, s)$ contenues
dans un jeu de données $\mathcal{D}$.
La forme habituellement retenue pour $\pi_\theta$ est celle d'un réseau de neurones
récurrent LSTM.

Note: le modèle doit fondamentalement contenir $\phi$ et $\pi$ car
l'encoder-décodeur optimal $\pi^*$ dépend du processus $\phi$ employé
pour la génération de résumés.

\todo{Figure tikz qui prend pas mal une page complète décrivant le processus de résumé extractif
    selon la formalisation donnée. Les étapes sont:
    \begin{itemize}
        \item Document $d$ en entrée, visuellement séparé pour voir les phrases.
        \item Application de $\pi_\theta$ sur le document: flèche indiquant
              explicitement que l'entrée est le document $d$ et la sortie est une distribution.
        \item Application de $\phi$ sur la distribution des phrases: flèche indiquant
              explicitement que l'entrée est une distribution sur les phrases et la sortie est un
              résumé.
        \item Évaluation de la qualité: Comparaison entre le résumé produit et le résumé cible $s$.
    \end{itemize}}

\subsubsection*{Approches supervisées}

La majorité des approches extractives sont supervisées.

Comme le résumé correspondant à un document n'est habituellement pas obtenu de
manière extractive (i.e. le résumé n'est pas une combinaison de phrases du document initial),
les approches supervisées doivent utilisent des cibles basées sur des heuristiques pour
leur entraînement.

Par exemple, \citet{10.5555/3298483.3298681} précalculent des cibles binaires $y_d \in \{0,1\}^{|d|}$
utilisées comme cibles pour chaque document $d$.
Leurs cibles binaires sont obtenues en sélectionnant de manière vorace
les phrases $d_i$ permettant de générer un résumé extractif aussi près que possible
du résumé cible $s$.
Après avoir sélectionné trois phrases, leur processus est arrêté et la cible $y_d$
est fixée à un vecteur binaire où les index des trois phrases retenues ont une valeur
de 1 et les autres index ont une valeur nulle.

L'emploi de cibles binaires permet un entraînement supervisé très efficace.
Le réseau entraîné sur ces cibles ne peut toutefois pas obtenir une performance
supérieure à celle de l'heuristique employée.
Il s'agit du principal facteur limitant de la performance de ces approches supervisées.

La performance est tout de même très bonne; \citep{zhong-etal-2020-extractive} est le SOTA
actuel en extractive.

\subsubsection*{Approches par renforcement}

Au lieu d'utiliser des cibles binaires, les approches par renforcement visent à
optimiser directement une mesure numérique de la similarité entre deux résumés.
Disons que l'on dispose d'une métrique numérique de performance $R(\hat{s}, s)$
permettant de quantifier la proximité entre un résumé produit $\hat{s}$ et un résumé
cible $s$.
Il est alors possible de définir la fonction objective à maximiser
\begin{equation}
    J(\theta) = \underset{(d,s) \sim \mathcal{D}}{\mathbb{E}} \; \underset{s \sim \phi(\pi_\theta(d))}{\mathbb{E}} \left[R(\hat{s}, s) \right],
    \label{eq:REINFORCE_expectation}
\end{equation}

représentant l'espérance de performance des résumés produits avec la paramétrisation $\theta$.
Comme on recherche $\theta^*$ maximisant $J(\theta)$, il vient naturellement en tête
de faire appel à des méthodes d'ascension de gradient.
Or, $J$ n'est pas calculable analytiquement en pratique car les deux espérances (sur
tous les documents et sur tous les résumés générables) sont habituellement intractables.


L'algorithme REINFORCE \citep{williams1992simple} propose une
solution élégante à ce problème, faisant une mise à jour des poids $\theta$ selon l'approximation du gradient
de $J(\theta)$
\begin{equation}
    \nabla J(\theta) = R(\hat{s}, s)\nabla \ln \pi_\theta (\hat{s}).
    \label{eq:REINFORCE_sample}
\end{equation}
Le théorème du gradient de politique \citep{sutton1999policy} affirme que, en espérance,
l'approximation \ref{eq:REINFORCE_sample} correspond au véritable gradient \ref{eq:REINFORCE_expectation}
et peut donc être utilisée pour une ascension de gradient.
Les approches par renforcement basées sur REINFORCE \citep{dong2018banditsum,luo-etal-2019-reading}
parviennent actuellement à atteindre des performances au niveau de l'état
de l'art pour la génération automatique de résumés.

Au niveau architectural, \citep{dong2018banditsum,luo-etal-2019-reading} emploient le
même modèle de réseau de neurones.
Comme encodeur, deux LSTMs consécutifs, un travaillant au niveau
des mots et l'autre au niveau des phrases.
Comme décodeur, une couche pleinement connectée partagée pour toutes les
phrases et avec sortie réelle.
Un schéma et davantage de détails sur ce modèle neuronal sont disponibles
au chapitre \ref{chap:bandit_contextuel}.
Nous reprendrons une architecture similaire de réseau de neurones pour toutes
les expérimentations.

\subsection{Formulation abstractive}

On écrit un résumé \textit{à la mitaine}.

Formulation difficile car nécessite de gérer la syntaxe et les fautes
d'orthographe en plus de la gestion du dilemme compression-conversation.

Récentes percées en NLG ont beaucoup boosté les performances ici.

\citep{2020t5, unilm, zhang2019pegasus} sont le SOTA en abstractif.

\subsection{Évaluation de la performance}
\label{sec:rouge}

À la section \ref{sec:extractive}, on prenait pour acquis qu'une fonction $R$ existait
pour distinguer quantitativement deux résumés candidats $s$ et $\hat{s}$.
Il en existe en fait plusieurs, notamment :

\begin{itemize}
    \item ROUGE-$n$ \citep{lin-2004-rouge}: métrique basée sur le chevauchement entre les séquences de $n$ mots (\ngrams)
          $s$ et $\hat{s}$;
    \item ROUGE-L \citep{lin-2004-rouge}: métrique mesurant la plus longue sous-séquence commune
          entre $s$ et $\hat{s}$;
    \item ROUGE-WE \citep{ng-abrecht-2015-better}: métrique similaire à ROUGE-$n$, mais qui utilise
          le \textit{soft-matching} basé sur la similarité cosine entre les plongements de mots au lieu
          du \textit{matching} exact.
\end{itemize}

Intuition: Plus le overlap
est grand entre les \ngrams d'un candidat et ceux de la cible, plus le résumé
a conservé l'information recherchée.

Un problème: si on se fie juste au rappel sur les \ngrams, le résumé optimal sera
de conserver tout le texte original. Pour incorporer la portion compression du dilemme
fondamental de la génération de résumé, on peut utiliser une métrique plus
appropriée comme le score F1 pour ajouter une pénalité sur les \ngrams présents dans
le candidat pas dans la cible.


\citep{peyrard-2019-studying} démontre que, bien que ces métriques soient généralement
corrélées avec l'avis d'un expert humain, elles peuvent difficilement être considérées
comme un remplacement de celui-ci.
En effet, comme les métriques sont toutes similairement corrélées avec le jugement humain
mais qu'elles ne sont que faiblement corrélées entre elles, aucune des métriques
ne peut être utilisée à elle seule comme un remplacement de l'avis humain.
En utilisant une moyenne de plusieurs métriques, il est possible d'alléger quelque peu
ce problème.
Notons toutefois que même l'utilisation d'une moyenne ne suffit pas à régler le problème:
\citep{DBLP:journals/corr/PaulusXS17} entraînent par
renforcement sur la formulation abstractive mais finissent par ne pas
utiliser le modèle avec le meilleure score employé (moyenne de ROUGE-1, ROUGE-2 et ROUGE-L).

\section{Formulation du problème retenue}

Pour les travaux présentés, il est choisi de s'intéresser exclusivement
à la génération automatique de résumés à partir d'un document en entrée.
Aussi, la formulation extractive est retenue en raison de la facilité avec
laquelle les approches par bandit peuvent y être utilisées.
Pour évaluer la performance des approches proposées, le jeu de données du
CNN/DailyMail \citep{10.5555/3298483.3298681}, référence dans le milieu
de la génération de résumés, sera utilisé.
Celui-ci est composé de plus de 300 000 articles de journaux et
leurs résumés écrits par un expert humain.

Nous emploierons aussi plusieurs des contraintes retenues par les approches
de l'état-de-l'art sur le jeu de données du CNN/DailyMail.
Notamment, comme \citep{dong2018banditsum}, nous considérerons seulement les résumés
de 3 phrases en sortie.
Cette contrainte, utilisée fréquemment sur le jeu de données du CNN/DailyMail,
est basée sur le fait que la moyenne du nombre de phrases contenues dans les résumés cibles
est de 3.
Aussi, une référence souvent utilisée pour ce jeu de données est l'heuristique \textit{Lead-3} \citep{10.5555/3298483.3298681}
qui consiste à générer le résumé d'un document en retenant les 3 premières phrases.
Les résultats obtenus par \textit{Lead-3} sont très bons aux yeux des métriques automatiques,
justifiant encore une fois qu'avec 3 phrases on peut générer de bons résumés des documents du jeu
de données.
Nous considérerons aussi la régularisation du jeu de données qui consiste à conserver seulement
les documents d'au moins 3 phrases, limiter la taille des documents à 50 phrases au maximum et
celle des phrases à 80 mots.
En cas d'excès de mots ou de phrases, l'excédent est simplement retiré.

Pour l'évaluation, on retient la métrique de similarité entre deux résumés
utilisée par la plupart des travaux employant l'apprentissage par renforcement \citep{dong2018banditsum,luo-etal-2019-reading}:
\begin{equation}
    \label{eq:ROUGE}
    R(\hat{s}, s) := \frac{1}{3} \left[ R_1(\hat{s}, s) + R_2(\hat{s}, s) + R_L(\hat{s}, s) \right].
\end{equation}

Ce choix de $R$ est motivé par:

\begin{itemize}
    \item Diversité de signal, comme expliqué plus haut.
    \item $R$ est \textbf{presque} invariante à l'ordre des phrases dans un résumé (seulement ROUGE-2
          n'est pas invariant, mais seulement au niveau de la jonction entre deux phrases). Cette propriété
          permet d'éviter l'explosion combinatoire induite par la considération de l'ordre des phrases dans les
          approches par bandits.
\end{itemize}
