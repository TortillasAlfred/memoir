\chapter{Génération automatique de résumés de textes}     % numéroté
\label{chap:generation_resumes}                           % étiquette pour renvois (à compléter!)

Quand on résume, on veut (1) compresser un ou des textes tout en
s'assurant de (2) conserver la majeure partie de l'information contenue.

On dit qu'il s'agit d'un dilemme compression-conservation.

Deux techniques principales actuellement utilisées: extractive et abstractive.

Nous nous intéresserons seulement au cas de la génération de résumés à partir
d'un seul document pour lequel nous possédons un seul résumé cible.

Notons que le cas où on génère un résumé à partir de plusieurs documents se ramène
naturellement au cas d'un seul document en considérant un nouveau document
représentant la concaténation de tous les documents en entrée.

\section{Formulation extractive}

On sélectionne des phrases du document initial.

Formulation simple à résoudre: la nombre de résumés dépend maintenant seulement du nombre
de phrases et on s'évite toute les difficultés en termes de syntaxe et de cohérence
d'avoir à générer du texte.

Les approches de l'état de l'art sont toutes basées sur une approche
encodeur-décodeur avec des réseaux de neurones.

Ces approches produisent en sortie une distribution sur les phrases
originales.

Le résumé est ensuite bâti à partir de la distribution prédite par le modèle.

Définition formelle: On dit qu'on a un modèle de génération de résumés
$(\pi, \phi)$.
Ici $\pi$ prend en entrée un document $d$ et retourne
$\pi(d)$, une distribution de probabilités de sélection sur les phrases
de $d$.
On a ensuite $\phi$, qui est un processus de génération de résumé
extractif à partir d'une distribution $\pi(d)$.
Deux exemples intuitifs de $\phi$ sont les processus voraces et stochastiques,
où on choisit les $n$ phrases avec la plus grande probabilité ou on en pige
$n$ sans remise de $\pi(d)$, respectivement.

Note: le modèle doit fondamentalement contenir $\phi$ et $\pi$ car
l'encoder-décodeur optimal $\pi^*$ dépend du processus $\phi$ employé
pour la génération de résumés.

\subsection{Approches supervisées}

La majorité des approches extractives sont supervisées.

Comme le résumé correspondant à un document n'est habituellement pas obtenu de
manière extractive (i.e. le résumé n'est pas une combinaison de phrases du document initial),
les approches supervisées doivent utilisent des cibles basées sur des heuristiques pour
leur entraînement.

Par exemple, \citep{10.5555/3298483.3298681} sélectionnent de manière vorace
des phrases une à la fois en fonction de leur similarité à un résumé pour
un document.

Après avoir sélectionné trois phrases, leur processus est arrêté et la cible pour le
document est fixée à un vecteur binaire où les index des trois phrases ont une valeur
de 1 et les autres index ont une valeur nulle.

Comme on est supervisé, le réseau entraîné ne peut être au mieux aussi bon que l'heuristique
utilisée pour générer les cibles utilisées.

La performance est tout de même très bonne; \citep{zhong-etal-2020-extractive} est le SOTA
actuel en extractive.

\subsection{Approches par renforcement}

\citep{dong2018banditsum,luo-etal-2019-reading}
sont des approches représentant l'état de l'art en la matière actuellement.

Au lieu d'utiliser des cibles binaires, les approches par renforcement visent à
optimiser directement une mesure de la similarité entre deux résumés: le ROUGE \citep{lin-2004-rouge}
(plus de détails plus bas).

\citep{dong2018banditsum,luo-etal-2019-reading} emploient le
même modèle de réseau de neurones.
Comme encodeur, deux LSTMs consécutifs, un travaillant au niveau
des mots et l'autre au niveau des phrases.
Comme décodeur, une couche pleinement connectée partagée pour toutes les
phrases et avec sortie réelle.
Un schéma et davantage de détails sur ce modèle neuronal sont disponibles
au chapitre \ref{chap:mcs}.
Nous reprendrons une architecture similaire de réseau de neurones pour toutes
les expérimentations.

Bémol à insérer : \citep{DBLP:journals/corr/PaulusXS17} entraînent par
renforcement sur la formulation abstractive mais finissent par ne pas
utiliser le modèle avec le meilleur score ROUGE.

\section{Formulation abstractive}

On écrit un résumé \textit{à la mitaine}.

Formulation difficile car nécessite de gérer la syntaxe et les fautes
d'orthographe en plus de la gestion du dilemme compression-conversation.

Récentes percées en NLG ont beaucoup boosté les performances ici.

\citep{2020t5, unilm, zhang2019pegasus} sont le SOTA en abstractif.

\section{Évaluation de la performance}

Comment distinguer quantitativement deux résumés candidats $s$ et $\hat{s}$ ?

Intuition: on se base sur les \ngrams d'un résumé cible. Plus le overlap
est grand entre les \ngrams d'un candidat et ceux de la cible, plus le résumé
a conservé l'information recherchée.

Un problème: si on se fie juste au rappel sur les \ngrams, le résumé optimal sera
de conserver tout le texte original. Pour incorporer la portion compression du dilemme
fondamental de la génération de résumé, on peut utiliser une métrique plus
appropriée comme le score F1 pour ajouter une pénalité sur les \ngrams présents dans
le candidat pas dans la cible.

Les considérations précédentes ont mené à la famille de métriques ROUGE \citep{lin-2004-rouge}.
Le score ROUGE correspond à assigner une valeur numérique à un résumé candidat à partir
de son F1-Score sur une tâche de classification sur les \ngrams d'un résumé cible.
En pratique, on utilise généralement $n={1,2}$ et on utilise aussi la plus longue
sous-séquence, nommée ROUGE-L.
On pose généralement

\begin{equation}
    \label{eq:ROUGE}
    R(s, \hat{s}) := \frac{1}{3} R_1(s, \hat{s}) + R_2(s, \hat{s}) + R_L(s, \hat{s}).
\end{equation}

On définit enfin la similarité entre un résumé candidat $s$ à partir d'un résumé cible $\hat{s}$
comme étant $R(s, \hat{s})$.

Une propriété intéressante du score ROUGE est son invariance à
l'ordre des phrases.

\todo{expliquer mérites et défauts de R1, R2 et RL et donc
    pourquoi leur moyenne est bonne. Range de R est [0,1]}

Benchmark classique: CNN/DailyMail \citep{DBLP:journals/corr/SeeLM17}.

À ajouter :

\begin{itemize}
    \item N articles (n train/valid/test)
    \item Moyennes mots, phrases texte/abstract
\end{itemize}

En fonction des stats, les modèles entraînés sur ce jeu de données produisent
pour la plupart des résumés de 3 phrases du document original.

C'est ce jeu de données qu'on utilisera pour tous les tests.

\subsection{Pré-calcul des scores ROUGE}

Les méthodes présentées dans les prochains chapitres utilisent toutes
un grand nombre de scores $R$ dans leur procédure d'entraînement.

Or, comme le calcul du ROUGE entre deux résumés est lent.
Aussi, il est possible de calculer, pour un document $d$ donné, tous les
résumés de 3 phrases possibles.

On a donc, dans un premier temps, précalculé tous les scores ROUGE
associés à tous les résumés pour chaque document du jeu de données.

\commentaire{Ce chapitre me semble un peu décousu, j'ai l'impression que
    plusieurs de mes sections ont des dépendances circulaires les unes envers les autres.}